{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /Users/silvanragettli/Python_Environments/snowcover-mapper/venv/bin/python\n",
      "Project root: /Users/silvanragettli/hydrosolutions Dropbox/Silvan Ragettli/2025_04_GlacierMapper/snowcover-mapper\n",
      "Python path includes project root: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Verify the environment is working\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path includes project root: {str(project_root) in sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "# # Authenticate and initialize Earth Engine\n",
    "# ee.Authenticate(auth_mode='notebook', force=True)\n",
    "# ee.Initialize(project=\"ee-sahellakes\")\n",
    "ee.Initialize(project=\"thurgau-irrigation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snow Cover Mapping Notebook\n",
    "\n",
    "# This cell initializes Earth Engine and sets up paths and config\n",
    "\n",
    "import geemap\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import your project-specific modules from `src`\n",
    "from src.modis_processing import (load_modis, fill_modis_with_aqua, add_date_bands,create_decadal_composites, extract_year_ranges,process_interval,process_interval_250,\n",
    "    load_modis_250,fill_modis_with_aqua_250, create_decadal_composites_250,modis_cloud_masking_250\n",
    ")\n",
    "from src.dem_processing import load_dem, classify_aspect, reproject_and_analyze_dem\n",
    "from src.snowline import get_snowline_elevation, calculate_glacier_metrics\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load AOI from RiverBasins_2023 asset\n",
    "# ---------------------------------------------\n",
    "RiverBasins_2023 = ee.FeatureCollection('users/hydrosolutions/RiverBasins_CA_Jan2023_simple1000')\n",
    "RiverBasins_2023 = RiverBasins_2023.map(lambda ft: ft.set('NAME', ee.String(ft.get('BASIN')).cat(ee.String('_')).cat(ee.String(ft.get('CODE')))))\n",
    "glims = ee.FeatureCollection(\"GLIMS/20230607\").filter(ee.Filter.eq('geog_area', \"Randolph Glacier Inventory; Umbrella RC for merging the RGI into GLIMS\"));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervals: 36\n",
      "intervals: [[{'type': 'Date', 'value': 1640995200000}, {'type': 'Date', 'value': 1641888000000}], [{'type': 'Date', 'value': 1641888000000}, {'type': 'Date', 'value': 1642780800000}], [{'type': 'Date', 'value': 1642780800000}, {'type': 'Date', 'value': 1643673600000}], [{'type': 'Date', 'value': 1643673600000}, {'type': 'Date', 'value': 1644480000000}], [{'type': 'Date', 'value': 1644480000000}, {'type': 'Date', 'value': 1645286400000}], [{'type': 'Date', 'value': 1645286400000}, {'type': 'Date', 'value': 1646092800000}], [{'type': 'Date', 'value': 1646092800000}, {'type': 'Date', 'value': 1646985600000}], [{'type': 'Date', 'value': 1646985600000}, {'type': 'Date', 'value': 1647878400000}], [{'type': 'Date', 'value': 1647878400000}, {'type': 'Date', 'value': 1648771200000}], [{'type': 'Date', 'value': 1648771200000}, {'type': 'Date', 'value': 1649635200000}], [{'type': 'Date', 'value': 1649635200000}, {'type': 'Date', 'value': 1650499200000}], [{'type': 'Date', 'value': 1650499200000}, {'type': 'Date', 'value': 1651363200000}], [{'type': 'Date', 'value': 1651363200000}, {'type': 'Date', 'value': 1652256000000}], [{'type': 'Date', 'value': 1652256000000}, {'type': 'Date', 'value': 1653148800000}], [{'type': 'Date', 'value': 1653148800000}, {'type': 'Date', 'value': 1654041600000}], [{'type': 'Date', 'value': 1654041600000}, {'type': 'Date', 'value': 1654905600000}], [{'type': 'Date', 'value': 1654905600000}, {'type': 'Date', 'value': 1655769600000}], [{'type': 'Date', 'value': 1655769600000}, {'type': 'Date', 'value': 1656633600000}], [{'type': 'Date', 'value': 1656633600000}, {'type': 'Date', 'value': 1657526400000}], [{'type': 'Date', 'value': 1657526400000}, {'type': 'Date', 'value': 1658419200000}], [{'type': 'Date', 'value': 1658419200000}, {'type': 'Date', 'value': 1659312000000}], [{'type': 'Date', 'value': 1659312000000}, {'type': 'Date', 'value': 1660204800000}], [{'type': 'Date', 'value': 1660204800000}, {'type': 'Date', 'value': 1661097600000}], [{'type': 'Date', 'value': 1661097600000}, {'type': 'Date', 'value': 1661990400000}], [{'type': 'Date', 'value': 1661990400000}, {'type': 'Date', 'value': 1662854400000}], [{'type': 'Date', 'value': 1662854400000}, {'type': 'Date', 'value': 1663718400000}], [{'type': 'Date', 'value': 1663718400000}, {'type': 'Date', 'value': 1664582400000}], [{'type': 'Date', 'value': 1664582400000}, {'type': 'Date', 'value': 1665475200000}], [{'type': 'Date', 'value': 1665475200000}, {'type': 'Date', 'value': 1666368000000}], [{'type': 'Date', 'value': 1666368000000}, {'type': 'Date', 'value': 1667260800000}], [{'type': 'Date', 'value': 1667260800000}, {'type': 'Date', 'value': 1668124800000}], [{'type': 'Date', 'value': 1668124800000}, {'type': 'Date', 'value': 1668988800000}], [{'type': 'Date', 'value': 1668988800000}, {'type': 'Date', 'value': 1669852800000}], [{'type': 'Date', 'value': 1669852800000}, {'type': 'Date', 'value': 1670745600000}], [{'type': 'Date', 'value': 1670745600000}, {'type': 'Date', 'value': 1671638400000}], [{'type': 'Date', 'value': 1671638400000}, {'type': 'Date', 'value': 1672531200000}]]\n"
     ]
    }
   ],
   "source": [
    "intervals = ee.List(extract_year_ranges(ee.List.sequence(2022, 2022), 10)\n",
    "    .iterate(lambda list, previous: ee.List(previous).cat(ee.List(list)), ee.List([])))\n",
    "print('intervals:',intervals.length().getInfo())\n",
    "print('intervals:',intervals.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOI: {'type': 'Polygon', 'coordinates': [[[78.35558680529317, 42.841624935346175], [78.34886246924157, 42.82958091911706], [78.38582393379298, 42.79144665039145], [78.38308162476883, 42.74938845973651], [78.36565543091028, 42.739132472737566], [78.36738999652627, 42.73449054294478], [78.40500703170778, 42.72508188230116], [78.40886416962566, 42.710366813467374], [78.42423912934738, 42.69919676146307], [78.42649543944474, 42.69001100182323], [78.45619304986197, 42.68484290924801], [78.45496678270888, 42.676847760794644], [78.53783915075857, 42.69240559491001], [78.62981709232788, 42.703526603101125], [78.64310515220191, 42.687103715906815], [78.6819796237287, 42.678399517588296], [78.7454326670329, 42.686457096467194], [78.79148623308848, 42.69238772002842], [78.87993245601564, 42.69640981460532], [78.87640979374024, 42.7201055241284], [78.92885325597925, 42.72002523156944], [78.94693050268994, 42.695950536158875], [78.95391786273467, 42.686341204648464], [78.99936502354535, 42.67334734660159], [79.03066347869523, 42.64089846674175], [79.0711075189675, 42.62741861327105], [79.07782290127369, 42.60883306864663], [79.08775780204903, 42.602554652550644], [79.07267715575715, 42.56971341532836], [79.07765793141856, 42.54048846953656], [79.08181829111045, 42.49874232781894], [79.08033344185863, 42.47459183968445], [79.08617479594446, 42.47316943574702], [79.0735332477332, 42.44011851502033], [79.05639253823847, 42.43174434635261], [79.06032985769784, 42.40351380449559], [79.06856139436574, 42.40188179670944], [79.07569594051378, 42.38595828607868], [79.12267257264047, 42.376380172586394], [79.13281255382874, 42.37907343804989], [79.15610688381967, 42.38058951619181], [79.16689790732637, 42.402104747133734], [79.1945131859213, 42.407723159614015], [79.206258444223, 42.4201551735482], [79.2135802630614, 42.420395921242125], [79.21412877192094, 42.43114236165979], [79.20484930734499, 42.433773234218435], [79.1931798880193, 42.46630240333518], [79.2028695424863, 42.473526158461056], [79.19456664485186, 42.48956551950469], [79.19733131697326, 42.50640307576659], [79.19015656872904, 42.53672943991156], [79.18667402298966, 42.545139308155164], [79.14852645176077, 42.56475044772917], [79.16514554435177, 42.62892133373523], [79.14605164342068, 42.668411178264584], [79.14670717160524, 42.68724196223586], [79.1299587340527, 42.7040304577211], [79.07169167510851, 42.72811407748902], [79.04013903844228, 42.75160016692267], [79.00649065490731, 42.76986905024564], [79.00282974302958, 42.77739158477781], [78.95678952406757, 42.77964341109778], [78.92491142874681, 42.77884081059366], [78.86002704576477, 42.80069484250122], [78.84124530988991, 42.797827645958115], [78.80261169238995, 42.80837342188169], [78.78553331210365, 42.80085091905284], [78.77077372977874, 42.81561947173746], [78.73609527840314, 42.82039963124027], [78.70474776784435, 42.82308843535467], [78.69476389550526, 42.81404536000539], [78.6622079990936, 42.816823442757], [78.64570037621027, 42.83622052219088], [78.63716567401748, 42.84414430725323], [78.60569779345342, 42.85401233224817], [78.58876214211143, 42.84734598253643], [78.56925354334342, 42.86338981333428], [78.55973784152995, 42.86508875489021], [78.54083565197142, 42.869382795964476], [78.52520652374065, 42.886889224362605], [78.5020147650544, 42.895129703308676], [78.49183015548478, 42.90048506275966], [78.47381990297184, 42.8947907860482], [78.4642907447993, 42.89648526883846], [78.4582263588598, 42.888895850228664], [78.42987096228846, 42.87955851976537], [78.38745600483058, 42.88217600113424], [78.37108664832029, 42.8871300543074], [78.35587660984324, 42.859595123345045], [78.35331713354026, 42.85080621561754], [78.35558680529317, 42.841624935346175]]]}\n",
      "interval: [{'type': 'Date', 'value': 1720684800000}, {'type': 'Date', 'value': 1721577600000}]\n",
      "aqua_ndsi: {'type': 'Image', 'bands': [{'id': 'sur_refl_b02', 'data_type': {'type': 'PixelType', 'precision': 'int', 'min': 0, 'max': 0}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}]}\n",
      "mscf 250: {'type': 'Image', 'bands': [{'id': 'sur_refl_b02', 'data_type': {'type': 'PixelType', 'precision': 'float', 'min': -3.2768001556396484, 'max': 3.276700019836426}, 'crs': 'SR-ORG:6974', 'crs_transform': [231.65635826395825, 0, -20015109.353988, 0, -231.65635826395834, 10007554.676994]}], 'properties': {'system:time_start': 951350400000, 'system:index': '2000_02_24'}}\n",
      "modis_ic 250: {'type': 'ImageCollection', 'bands': [], 'features': [{'type': 'Image', 'bands': [{'id': 'value', 'data_type': {'type': 'PixelType', 'precision': 'float', 'min': -3.2768001556396484, 'max': 3.276700019836426}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}], 'properties': {'system:time_start': 1720656000000, 'Year-Month-Day': '2024-07-11', 'system:index': '0'}}]}\n",
      "sample_img: {'type': 'Image', 'bands': [{'id': 'value', 'data_type': {'type': 'PixelType', 'precision': 'double', 'min': 0, 'max': 255}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}], 'properties': {'system:time_start': 1720656000000, 'Year-Month-Day': '2024-07-11', 'system:index': '0'}}\n",
      "Snowline elevation by aspect: {'type': 'Feature', 'geometry': None, 'properties': {'East': 4127.176967399908, 'North': 4127.176967399908, 'South': 4127.176967399908, 'West': 4127.176967399908, 'mixed': 4127.176967399908}}\n",
      "Fractional Snow Cover 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# filter by code 16230\n",
    "aoi = RiverBasins_2023.filter(ee.Filter.eq('CODE', '15002')).geometry()\n",
    "print('AOI:', aoi.getInfo())\n",
    "\n",
    "start_year = 2024\n",
    "end_year = 2024\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load and process MODIS\n",
    "# ---------------------------------------------\n",
    "intervals = ee.List(extract_year_ranges(ee.List.sequence(start_year, end_year), 10)\n",
    "    .iterate(lambda list, previous: ee.List(previous).cat(ee.List(list)), ee.List([])))\n",
    "\n",
    "# print('intervals:',intervals)\n",
    "print('interval:', ee.List(intervals.slice(19,20).get(0)).getInfo())\n",
    "\n",
    "# Load all MODIS data for the entire time span\n",
    "start_date = ee.Date.fromYMD(start_year, 1, 1)\n",
    "end_date = ee.Date.fromYMD(end_year, 12, 31)\n",
    "terra_coll = load_modis(aoi)\n",
    "\n",
    "# Create filled MODIS snow cover fraction collection\n",
    "mscf = terra_coll.map(lambda img: fill_modis_with_aqua(img))\n",
    "# print('mscf:', mscf.first().getInfo())\n",
    "\n",
    "# time_intervals_all should be an ee.List of [start, end] ee.Date pairs\n",
    "modis_ic = ee.ImageCollection(intervals.slice(19,20).map(lambda list:process_interval(mscf, list)))\n",
    "# print('modis_ic:', modis_ic.filterBounds(aoi).getInfo())\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load MODIS 250 m data (only for testing)\n",
    "# ---------------------------------------------\n",
    "terra_coll = load_modis_250(aoi)\n",
    "# mscf = terra_coll.map(lambda img: modis_cloud_masking_250(img,aoi))\n",
    "# print('mscf 250:', mscf.first().getInfo())\n",
    "# # time_intervals_all should be an ee.List of [start, end] ee.Date pairs\n",
    "# modis_ic_250 = ee.ImageCollection(intervals.slice(19,20).map(lambda list:process_interval_250(mscf, list)))\n",
    "# print('modis_ic 250:', modis_ic_250.filterBounds(aoi).getInfo())\n",
    "\n",
    "mscf = terra_coll.map(lambda img: fill_modis_with_aqua_250(img,aoi))\n",
    "print('mscf 250:', mscf.first().getInfo())\n",
    "# time_intervals_all should be an ee.List of [start, end] ee.Date pairs\n",
    "modis_ic_250 = ee.ImageCollection(intervals.slice(19,20).map(lambda list:process_interval_250(mscf, list)))\n",
    "print('modis_ic 250:', modis_ic_250.filterBounds(aoi).getInfo())\n",
    "\n",
    "# modis_ic = create_decadal_composites(aoi, start_year, end_year, agg_interval=10)\n",
    "# print('modis_ic:', modis_ic.filterBounds(aoi).first().getInfo())\n",
    "# terra, aqua = load_modis(aoi, start_date, end_date)\n",
    "# filled_modis = terra.map(lambda img: fill_modis_with_aqua(img, aqua))\n",
    "# filled_modis = filled_modis.map(add_date_bands)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load DEM and classify terrain aspect\n",
    "# ---------------------------------------------\n",
    "dem = load_dem()\n",
    "modisProjection = modis_ic.filterBounds(aoi).first().projection()\n",
    "scale = 500\n",
    "tile_scale = 2\n",
    "sc_th = 50\n",
    "aspect_keys = ['East', 'North', 'South', 'West', 'mixed']\n",
    "# Reproject and analyze DEM\n",
    "reprojected_dem, min_dem_dict, max_dem_dict, n_grid = reproject_and_analyze_dem(dem, modisProjection, aoi, scale, tile_scale, aspect_keys)\n",
    "# print('reprojected_dem:', reprojected_dem.getInfo())\n",
    "\n",
    "aspects, aspect_coded = classify_aspect(dem, modisProjection, scale)\n",
    "# print('aspects:', aspects.bandNames().getInfo())\n",
    "# print('n_grid:', n_grid.getInfo())\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Snowline elevation estimation from first MODIS image\n",
    "# ---------------------------------------------\n",
    "sample_img = ee.Image(modis_ic.filterDate(start_date, end_date).first())\n",
    "\n",
    "# Test on a problematic summer image\n",
    "sample_img = modis_ic.filterDate('2024-07-01', '2024-08-01').first()\n",
    "\n",
    "print('sample_img:', sample_img.getInfo())\n",
    "# Get snowline elevation and statistics\n",
    "snowline_stats,fsc = get_snowline_elevation(sample_img, reprojected_dem, aspect_coded, aoi,min_dem_dict, max_dem_dict,n_grid, \n",
    "                                                      scale=500, scale_dem=500, sc_th=50, canny_threshold=0.7, \n",
    "                           canny_sigma=0.7, ppha=10, tile_scale=1, point2sample=1000, aspectKeys=['East', 'North', 'South', 'West', 'mixed'])\n",
    "  \n",
    "print('Snowline elevation by aspect:', snowline_stats.getInfo())\n",
    "print('Fractional Snow Cover', fsc.getInfo())\n",
    "\n",
    "glacier_metrics=calculate_glacier_metrics(glims, aoi, sample_img,sc_th, snowline_stats , dem, aspect_keys, tile_scale, aspects)\n",
    "# Access individual EE values from the dictionary and get their info\n",
    "# print('Glacier SCF:', glacier_metrics['glims_fsc'].getInfo())\n",
    "# print('Glacier SCF below snowline:', glacier_metrics['glims_fsc_below_sl'].getInfo())\n",
    "# print('Glacier area below snowline:', glacier_metrics['glims_area_below_sl'].getInfo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing MODIS 500 NSDI DATA PER BASIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing catchment ISSYKUL_15054...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15002...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15013...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15016...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15020...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15022...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15025...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15030...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15034...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15039...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15040...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15044...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15045...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15049...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15051...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15057...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15064...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15069...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15070...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15081...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15083...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15090...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17461...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17043...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17045...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17047...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17050...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17058...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17059...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17062...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17073...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17078...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17082...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17100...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17101...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17110...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17137...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17150...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17185...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17202...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17288...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17325...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17329...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17338...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17344...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17391...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17453...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17459...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17462...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_16223...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SURKHANDARYA_17194...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SURKHANDARYA_17211...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SURKHANDARYA_17223...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17231...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17236...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17257...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17260...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17275...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17279...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17464...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-0.000-4M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-0.000-6M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-1.1L0-7A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-0.000-3M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-1.L00-1T...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_11-0.000-4M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_11-1.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment BALKH_12-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment BALKH_12-0.000-9M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment BALKH_12-0.000-10M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment BALKH_12-1.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KHULM_13-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KHULM_13-0.000-2M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-1.1L0-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-0.000-3M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-0.000-6M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-1.L00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-10.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-10.R00-2A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-2M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-3M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-4M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-5S...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-6M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-8M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-1.R00-2A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-1.R00-5A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-5.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-9.5R0-1T...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-9.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-9.R00-6T...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-9.R00-8A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17089...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17102...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17106...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17107...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17109...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17111...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17113...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17114...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17115...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17116...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17119...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17121...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17122...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17125...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17126...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17141...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17147...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17157...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17160...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17165...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17169...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17170...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17172...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17176...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17177...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17178...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17179...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17324...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17333...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17335...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17341...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17342...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17343...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_70003...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17052...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17053...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17057...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17061...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17063...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17064...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17065...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17067...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17068...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17072...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17074...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17077...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17081...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_70001...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_70002...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15102...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15149...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15171...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15189...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15194...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15212...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15214...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15215...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15216...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15256...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15259...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15261...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15278...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15283...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15285...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15287...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15290...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15292...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15312...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16133...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16936...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16326...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16340...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16363...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16374...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16375...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16390...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16055...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16059...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16068...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16070...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16076...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16101...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16121...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16127...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16134...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16135...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16136...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16139...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16143...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16146...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16151...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16153...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16158...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16159...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16169...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16176...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16487...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16510...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16107...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16262...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16279...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16290...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16298...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16300...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16924...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16938...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60048...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60032...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60033...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60034...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60035...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60036...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60037...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60038...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60039...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60040...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60041...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60042...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60043...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60044...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60045...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60046...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_16230...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16057...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16072...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16074...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16075...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16080...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16081...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16082...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16085...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16087...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16088...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16093...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16096...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16100...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16103...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16105...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16124...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16137...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16154...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16155...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16160...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16161...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16162...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16163...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16164...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16175...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16187...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16192...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16193...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16198...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16202...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16205...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16210...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16211...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60002...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60003...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60005...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60006...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60007...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60008...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60009...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60010...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60011...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60012...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60013...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60014...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60015...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60016...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60017...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60018...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60019...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60020...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60021...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60022...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60023...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60024...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60025...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60026...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60029...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60031...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-0.000-5M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-1.000-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-2.000-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-3.000-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-4.R00-8A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-5.L00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-2M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-3S...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-4M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-5M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-7M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-9M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-1.R00-9T...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-3.L00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-3.L00-6A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-11.L00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-2.R00-3A...\n",
      "Export started for year 2025 and month 8.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "##todo: automatically identify the last export date and continue from there\n",
    "\n",
    "# Loop over each basin and export results for a given month (or several months)\n",
    "year=2025\n",
    "catchment_names=RiverBasins_2023.aggregate_array('NAME').getInfo()\n",
    "#get the current month\n",
    "current_month = datetime.datetime.now().month  # returns 1-12\n",
    "\n",
    "for catchment_name in catchment_names:\n",
    "\n",
    "    print(f\"Processing catchment {catchment_name}...\")\n",
    "    \n",
    "    # filter by code 16230\n",
    "    aoi = RiverBasins_2023.filter(ee.Filter.eq('NAME', catchment_name)).geometry()\n",
    "\n",
    "    # Load MODIS data for the year\n",
    "    modis_ic = create_decadal_composites(aoi, year, year, agg_interval=10)\n",
    "\n",
    "    # Define start and end dates for the year: run from the beginning to the previous month from now\n",
    "    start_date = ee.Date.fromYMD(year, 1, 1)\n",
    "    end_date = ee.Date.fromYMD(year, current_month - 1, 31)\n",
    "    \n",
    "    # Filter by date\n",
    "    modis_ic = ee.ImageCollection(modis_ic).filter(ee.Filter.And(ee.Filter.calendarRange(year, year, 'year'), ee.Filter.calendarRange(1, current_month - 1, 'month')))\n",
    "    modisProjection = modis_ic.filterBounds(aoi).first().projection()\n",
    "    reprojected_dem, min_dem_dict, max_dem_dict, n_grid = reproject_and_analyze_dem(dem, modisProjection, aoi, scale, tile_scale, aspect_keys)\n",
    "    aspects, aspect_coded = classify_aspect(dem, modisProjection, scale)    \n",
    "    \n",
    "    # Function to process each image and create a feature with properties\n",
    "    def create_feature_with_properties(img):\n",
    "        # Get snowline elevation for this image\n",
    "        current_snowline_stats, current_fsc = get_snowline_elevation(\n",
    "            img, reprojected_dem, aspect_coded, aoi, min_dem_dict, max_dem_dict, n_grid,\n",
    "            scale=500, scale_dem=500, sc_th=50, canny_threshold=0.7,\n",
    "            canny_sigma=0.7, ppha=10, tile_scale=1, point2sample=1000, \n",
    "            aspectKeys=aspect_keys\n",
    "        )\n",
    "        \n",
    "        # Calculate glacier metrics\n",
    "        current_glacier_metrics = calculate_glacier_metrics(\n",
    "            glims, aoi, img, sc_th, current_snowline_stats, dem, aspect_keys, tile_scale, aspects\n",
    "        )\n",
    "        \n",
    "        # Create a dictionary with aspect-specific values\n",
    "        aspect_dict = {}\n",
    "        base = ee.String('SLA_')\n",
    "        \n",
    "        # Add properties for each aspect (excluding 'mixed')\n",
    "        for i, aspect in enumerate(aspect_keys[:-1]):\n",
    "            aspect_dict[base.cat(aspect)] = current_snowline_stats.get(aspect)\n",
    "        \n",
    "        # Get date info\n",
    "        img_date = ee.Date(img.get('system:time_start'))\n",
    "        img_year = img_date.get('year')\n",
    "        img_decade = ee.Number(img_date.get('day')).add(2).divide(10).ceil()\n",
    "        \n",
    "        # Create feature with all properties\n",
    "        feature = ee.Feature(None).set(\n",
    "            'Year-Month-Day', img_date.format('YYYY-MM-dd'),\n",
    "            'year', img_year,\n",
    "            'decade', img_decade,\n",
    "            'gla_fsc', current_glacier_metrics['glims_fsc'],\n",
    "            'gla_fsc_below_sl50', current_glacier_metrics['glims_fsc_below_sl'],\n",
    "            'gla_area_below_sl50', current_glacier_metrics['glims_area_below_sl'],\n",
    "            'fsc', current_fsc\n",
    "        )\n",
    "        \n",
    "        # Add aspect-specific properties\n",
    "        for key, value in aspect_dict.items():\n",
    "            feature = feature.set(key, value)\n",
    "            \n",
    "        return feature\n",
    "    \n",
    "    # Apply the function to each image in the collection\n",
    "    aoi_mean_tmp = modis_ic.map(create_feature_with_properties)\n",
    "    \n",
    "    # Add geometry to features (because null geometry can't be exported)\n",
    "    joined = aoi_mean_tmp.map(lambda ft: ee.Feature(aoi.centroid(1000)).copyProperties(ft))\n",
    "    \n",
    "    # Sort by glacier snow cover below snowline\n",
    "    table_to_export = joined#.sort('gla_fsc_below_sl50', False)\n",
    "    \n",
    "    export_layer_name = 'decadal_SLA'  # Modify as needed\n",
    "    \n",
    "    # Create year_month string for asset naming\n",
    "    year_month = f\"{year}_{current_month-1:02d}\"\n",
    "    \n",
    "    # Export to asset\n",
    "    task = ee.batch.Export.table.toAsset(\n",
    "        collection=ee.FeatureCollection(table_to_export).set('NAME', catchment_name),\n",
    "        description=f\"{export_layer_name}_{catchment_name.replace('.', '')}_{year_month}\",\n",
    "        assetId=f\"projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/{export_layer_name}_{catchment_name.replace('.', '')}_{year_month}\"\n",
    "    )\n",
    "    \n",
    "    # # Start the export task\n",
    "    task.start()\n",
    "    print(f\"Export started for year {year} and month {current_month-1}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through years 2001 to 2024 (annual assets)\n",
    "for year in range(start_year, end_year + 1):\n",
    "\n",
    "    print(f\"Processing year {year}...\")\n",
    "    \n",
    "    # Define start and end dates for the year\n",
    "    start_date = ee.Date.fromYMD(year, 1, 1)\n",
    "    end_date = ee.Date.fromYMD(year, 12, 31)\n",
    "    \n",
    "    # Load MODIS data for the year\n",
    "    modis_ic = create_decadal_composites(aoi, year, year, agg_interval=10)\n",
    "    \n",
    "    # Load and process DEM if needed (or reuse from outside the loop if not changing)\n",
    "    dem = load_dem()\n",
    "    modisProjection = modis_ic.filterBounds(aoi).first().projection()\n",
    "    \n",
    "    # Function to process each image and create a feature with properties\n",
    "    def create_feature_with_properties(img):\n",
    "        # Get snowline elevation for this image\n",
    "        current_snowline_stats, current_fsc = get_snowline_elevation(\n",
    "            img, reprojected_dem, aspect_coded, aoi, min_dem_dict, max_dem_dict, n_grid,\n",
    "            scale=500, scale_dem=500, sc_th=50, canny_threshold=0.7,\n",
    "            canny_sigma=0.7, ppha=10, tile_scale=1, point2sample=1000, \n",
    "            aspectKeys=aspect_keys\n",
    "        )\n",
    "        \n",
    "        # Calculate glacier metrics\n",
    "        current_glacier_metrics = calculate_glacier_metrics(\n",
    "            glims, aoi, img, sc_th, current_snowline_stats, dem, aspect_keys, tile_scale, aspects\n",
    "        )\n",
    "        \n",
    "        # Create a dictionary with aspect-specific values\n",
    "        aspect_dict = {}\n",
    "        base = ee.String('SLA_')\n",
    "        \n",
    "        # Add properties for each aspect (excluding 'mixed')\n",
    "        for i, aspect in enumerate(aspect_keys[:-1]):\n",
    "            aspect_dict[base.cat(aspect)] = current_snowline_stats.get(aspect)\n",
    "        \n",
    "        # Get date info\n",
    "        img_date = ee.Date(img.get('system:time_start'))\n",
    "        img_year = img_date.get('year')\n",
    "        img_decade = ee.Number(img_date.get('day')).add(2).divide(10).ceil()\n",
    "        \n",
    "        # Create feature with all properties\n",
    "        feature = ee.Feature(None).set(\n",
    "            'Year-Month-Day', img_date.format('YYYY-MM-dd'),\n",
    "            'year', img_year,\n",
    "            'decade', img_decade,\n",
    "            'gla_fsc', current_glacier_metrics['glims_fsc'],\n",
    "            'gla_fsc_below_sl50', current_glacier_metrics['glims_fsc_below_sl'],\n",
    "            'gla_area_below_sl50', current_glacier_metrics['glims_area_below_sl'],\n",
    "            'fsc', current_fsc\n",
    "        )\n",
    "        \n",
    "        # Add aspect-specific properties\n",
    "        for key, value in aspect_dict.items():\n",
    "            feature = feature.set(key, value)\n",
    "            \n",
    "        return feature\n",
    "    \n",
    "    # Apply the function to each image in the collection\n",
    "    aoi_mean_tmp = modis_ic.map(create_feature_with_properties)\n",
    "    \n",
    "    # Add geometry to features (because null geometry can't be exported)\n",
    "    joined = aoi_mean_tmp.map(lambda ft: ee.Feature(aoi.centroid(1000)).copyProperties(ft))\n",
    "    \n",
    "    # Sort by glacier snow cover below snowline\n",
    "    table_to_export = joined#.sort('gla_fsc_below_sl50', False)\n",
    "    \n",
    "    # Set catchment name (assuming it's defined elsewhere)\n",
    "    catchment_name = 'AKHANGARAN_16230'  # Modify as needed\n",
    "    export_layer_name = 'decadal_SLA'  # Modify as needed\n",
    "    \n",
    "    # Export to asset\n",
    "    task = ee.batch.Export.table.toAsset(\n",
    "        collection=ee.FeatureCollection(table_to_export).set('NAME', catchment_name),\n",
    "        description=f\"{export_layer_name}_{catchment_name.replace('.', '')}_Year{year}\",\n",
    "        assetId=f\"projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/{export_layer_name}_{catchment_name.replace('.', '')}_Year{year}\"\n",
    "    )\n",
    "    \n",
    "    # Start the export task\n",
    "    task.start()\n",
    "    print(f\"Export started for year {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODIS 250m REFLECTANCE TIME SERIES OF MEAN VALUES OVER GLACIERIZED AREA, PER BASIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catchment_names: ['ISSYKUL_15054', 'ISSYKUL_15002', 'ISSYKUL_15013', 'ISSYKUL_15016', 'ISSYKUL_15020', 'ISSYKUL_15022', 'ISSYKUL_15025', 'ISSYKUL_15030', 'ISSYKUL_15034', 'ISSYKUL_15039', 'ISSYKUL_15040', 'ISSYKUL_15044', 'ISSYKUL_15045', 'ISSYKUL_15049', 'ISSYKUL_15051', 'ISSYKUL_15057', 'ISSYKUL_15064', 'ISSYKUL_15069', 'ISSYKUL_15070', 'ISSYKUL_15081', 'ISSYKUL_15083', 'ISSYKUL_15090', 'ZERAFSHAN_17461', 'PYANDZH_17043', 'PYANDZH_17045', 'PYANDZH_17047', 'PYANDZH_17050', 'PYANDZH_17058', 'PYANDZH_17059', 'PYANDZH_17062', 'PYANDZH_17073', 'PYANDZH_17078', 'VAKSH_17082', 'VAKSH_17100', 'VAKSH_17101', 'VAKSH_17110', 'KOFARNIKHAN_17137', 'KOFARNIKHAN_17150', 'KOFARNIKHAN_17185', 'KOFARNIKHAN_17202', 'ZERAFSHAN_17288', 'ZERAFSHAN_17325', 'ZERAFSHAN_17329', 'ZERAFSHAN_17338', 'ZERAFSHAN_17344', 'PYANDZH_17391', 'PYANDZH_17453', 'VAKSH_17459', 'VAKSH_17462', 'ZERAFSHAN_16223', 'SURKHANDARYA_17194', 'SURKHANDARYA_17211', 'SURKHANDARYA_17223', 'QASHQADARYA_17231', 'QASHQADARYA_17236', 'QASHQADARYA_17257', 'QASHQADARYA_17260', 'QASHQADARYA_17275', 'QASHQADARYA_17279', 'QASHQADARYA_17464', 'SHIRINTAGAB_10-0.000-4M', 'SHIRINTAGAB_10-0.000-6M', 'SHIRINTAGAB_10-1.1L0-7A', 'SHIRINTAGAB_10-0.000-3M', 'SHIRINTAGAB_10-1.L00-1T', 'SHIRINTAGAB_11-0.000-4M', 'SHIRINTAGAB_11-1.R00-1A', 'BALKH_12-0.000-1M', 'BALKH_12-0.000-9M', 'BALKH_12-0.000-10M', 'BALKH_12-1.R00-1A', 'KHULM_13-0.000-1M', 'KHULM_13-0.000-2M', 'KUNDUZ_14-1.1L0-1A', 'KOKCHA_15-0.000-1M', 'KOKCHA_15-0.000-3M', 'KOKCHA_15-0.000-6M', 'KOKCHA_15-1.L00-1A', 'KOKCHA_15-10.R00-1A', 'KOKCHA_15-10.R00-2A', 'KUNDUZ_14-0.000-1M', 'KUNDUZ_14-0.000-2M', 'KUNDUZ_14-0.000-3M', 'KUNDUZ_14-0.000-4M', 'KUNDUZ_14-0.000-5S', 'KUNDUZ_14-0.000-6M', 'KUNDUZ_14-0.000-8M', 'KUNDUZ_14-1.R00-2A', 'KUNDUZ_14-1.R00-5A', 'KUNDUZ_14-5.R00-1A', 'KUNDUZ_14-9.5R0-1T', 'KUNDUZ_14-9.R00-1A', 'KUNDUZ_14-9.R00-6T', 'KUNDUZ_14-9.R00-8A', 'VAKSH_17089', 'VAKSH_17102', 'VAKSH_17106', 'VAKSH_17107', 'VAKSH_17109', 'VAKSH_17111', 'VAKSH_17113', 'VAKSH_17114', 'VAKSH_17115', 'VAKSH_17116', 'VAKSH_17119', 'VAKSH_17121', 'VAKSH_17122', 'KOFARNIKHAN_17125', 'KOFARNIKHAN_17126', 'KOFARNIKHAN_17141', 'KOFARNIKHAN_17147', 'KOFARNIKHAN_17157', 'KOFARNIKHAN_17160', 'KOFARNIKHAN_17165', 'KOFARNIKHAN_17169', 'KOFARNIKHAN_17170', 'KOFARNIKHAN_17172', 'KOFARNIKHAN_17176', 'KOFARNIKHAN_17177', 'KOFARNIKHAN_17178', 'KOFARNIKHAN_17179', 'ZERAFSHAN_17324', 'ZERAFSHAN_17333', 'ZERAFSHAN_17335', 'ZERAFSHAN_17341', 'ZERAFSHAN_17342', 'ZERAFSHAN_17343', 'ZERAFSHAN_70003', 'PYANDZH_17052', 'PYANDZH_17053', 'PYANDZH_17057', 'PYANDZH_17061', 'PYANDZH_17063', 'PYANDZH_17064', 'PYANDZH_17065', 'PYANDZH_17067', 'PYANDZH_17068', 'PYANDZH_17072', 'PYANDZH_17074', 'PYANDZH_17077', 'PYANDZH_17081', 'PYANDZH_70001', 'PYANDZH_70002', 'CHU_15102', 'CHU_15149', 'CHU_15171', 'CHU_15189', 'CHU_15194', 'CHU_15212', 'CHU_15214', 'CHU_15215', 'CHU_15216', 'TALAS_15256', 'TALAS_15259', 'TALAS_15261', 'TALAS_15278', 'TALAS_15283', 'TALAS_15285', 'TALAS_15287', 'TALAS_15290', 'TALAS_15292', 'TALAS_15312', 'SYR_DARYA_16133', 'NARYN_16936', 'ARYS_16326', 'ARYS_16340', 'ARYS_16363', 'ARYS_16374', 'ARYS_16375', 'ARYS_16390', 'NARYN_16055', 'NARYN_16059', 'NARYN_16068', 'NARYN_16070', 'NARYN_16076', 'SYR_DARYA_16101', 'SYR_DARYA_16121', 'SYR_DARYA_16127', 'SYR_DARYA_16134', 'SYR_DARYA_16135', 'SYR_DARYA_16136', 'SYR_DARYA_16139', 'SYR_DARYA_16143', 'SYR_DARYA_16146', 'SYR_DARYA_16151', 'SYR_DARYA_16153', 'SYR_DARYA_16158', 'SYR_DARYA_16159', 'SYR_DARYA_16169', 'SYR_DARYA_16176', 'SYR_DARYA_16487', 'SYR_DARYA_16510', 'SYR_DARYA_16107', 'CHIRCHIK_16262', 'CHIRCHIK_16279', 'CHIRCHIK_16290', 'CHIRCHIK_16298', 'CHIRCHIK_16300', 'CHIRCHIK_16924', 'SYR_DARYA_16938', 'CHIRCHIK_60048', 'CHIRCHIK_60032', 'CHIRCHIK_60033', 'CHIRCHIK_60034', 'CHIRCHIK_60035', 'CHIRCHIK_60036', 'CHIRCHIK_60037', 'CHIRCHIK_60038', 'CHIRCHIK_60039', 'CHIRCHIK_60040', 'CHIRCHIK_60041', 'AKHANGARAN_60042', 'AKHANGARAN_60043', 'AKHANGARAN_60044', 'AKHANGARAN_60045', 'AKHANGARAN_60046', 'AKHANGARAN_16230', 'NARYN_16057', 'NARYN_16072', 'NARYN_16074', 'NARYN_16075', 'NARYN_16080', 'NARYN_16081', 'NARYN_16082', 'NARYN_16085', 'NARYN_16087', 'NARYN_16088', 'NARYN_16093', 'NARYN_16096', 'NARYN_16100', 'NARYN_16103', 'NARYN_16105', 'SYR_DARYA_16124', 'SYR_DARYA_16137', 'SYR_DARYA_16154', 'SYR_DARYA_16155', 'SYR_DARYA_16160', 'SYR_DARYA_16161', 'SYR_DARYA_16162', 'SYR_DARYA_16163', 'SYR_DARYA_16164', 'SYR_DARYA_16175', 'SYR_DARYA_16187', 'SYR_DARYA_16192', 'SYR_DARYA_16193', 'SYR_DARYA_16198', 'SYR_DARYA_16202', 'SYR_DARYA_16205', 'SYR_DARYA_16210', 'SYR_DARYA_16211', 'NARYN_60002', 'NARYN_60003', 'NARYN_60005', 'NARYN_60006', 'NARYN_60007', 'NARYN_60008', 'NARYN_60009', 'NARYN_60010', 'NARYN_60011', 'NARYN_60012', 'NARYN_60013', 'NARYN_60014', 'NARYN_60015', 'NARYN_60016', 'NARYN_60017', 'NARYN_60018', 'NARYN_60019', 'NARYN_60020', 'NARYN_60021', 'SYR_DARYA_60022', 'SYR_DARYA_60023', 'SYR_DARYA_60024', 'SYR_DARYA_60025', 'SYR_DARYA_60026', 'SYR_DARYA_60029', 'SYR_DARYA_60031', 'MURGHAB_9-0.000-1M', 'MURGHAB_9-0.000-5M', 'MURGHAB_9-1.000-1A', 'MURGHAB_9-2.000-1A', 'MURGHAB_9-3.000-1A', 'MURGHAB_9-4.R00-8A', 'MURGHAB_9-5.L00-1A', 'HARIRUD_8-0.000-1M', 'HARIRUD_8-0.000-2M', 'HARIRUD_8-0.000-3S', 'HARIRUD_8-0.000-4M', 'HARIRUD_8-0.000-5M', 'HARIRUD_8-0.000-7M', 'HARIRUD_8-0.000-9M', 'HARIRUD_8-1.R00-9T', 'HARIRUD_8-3.L00-1A', 'HARIRUD_8-3.L00-6A', 'HARIRUD_8-11.L00-1A', 'HARIRUD_8-2.R00-3A']\n",
      "position: 199\n",
      "position: 271\n"
     ]
    }
   ],
   "source": [
    "# Check position of catchment in the list\n",
    "catchment_names=RiverBasins_2023.aggregate_array('NAME').getInfo()\n",
    "print('catchment_names:', catchment_names)\n",
    "#position of the catchment with id 16205 in NAME\n",
    "position=catchment_names.index('SYR_DARYA_16938')\n",
    "print('position:', position)\n",
    "position=catchment_names.index('SYR_DARYA_60024')\n",
    "print('position:', position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catchment_names: ['ISSYKUL_15054', 'ISSYKUL_15002', 'ISSYKUL_15013', 'ISSYKUL_15016', 'ISSYKUL_15020', 'ISSYKUL_15022', 'ISSYKUL_15025', 'ISSYKUL_15030', 'ISSYKUL_15034', 'ISSYKUL_15039', 'ISSYKUL_15040', 'ISSYKUL_15044', 'ISSYKUL_15045', 'ISSYKUL_15049', 'ISSYKUL_15051', 'ISSYKUL_15057', 'ISSYKUL_15064', 'ISSYKUL_15069', 'ISSYKUL_15070', 'ISSYKUL_15081', 'ISSYKUL_15083', 'ISSYKUL_15090', 'ZERAFSHAN_17461', 'PYANDZH_17043', 'PYANDZH_17045', 'PYANDZH_17047', 'PYANDZH_17050', 'PYANDZH_17058', 'PYANDZH_17059', 'PYANDZH_17062', 'PYANDZH_17073', 'PYANDZH_17078', 'VAKSH_17082', 'VAKSH_17100', 'VAKSH_17101', 'VAKSH_17110', 'KOFARNIKHAN_17137', 'KOFARNIKHAN_17150', 'KOFARNIKHAN_17185', 'KOFARNIKHAN_17202', 'ZERAFSHAN_17288', 'ZERAFSHAN_17325', 'ZERAFSHAN_17329', 'ZERAFSHAN_17338', 'ZERAFSHAN_17344', 'PYANDZH_17391', 'PYANDZH_17453', 'VAKSH_17459', 'VAKSH_17462', 'ZERAFSHAN_16223', 'SURKHANDARYA_17194', 'SURKHANDARYA_17211', 'SURKHANDARYA_17223', 'QASHQADARYA_17231', 'QASHQADARYA_17236', 'QASHQADARYA_17257', 'QASHQADARYA_17260', 'QASHQADARYA_17275', 'QASHQADARYA_17279', 'QASHQADARYA_17464', 'SHIRINTAGAB_10-0.000-4M', 'SHIRINTAGAB_10-0.000-6M', 'SHIRINTAGAB_10-1.1L0-7A', 'SHIRINTAGAB_10-0.000-3M', 'SHIRINTAGAB_10-1.L00-1T', 'SHIRINTAGAB_11-0.000-4M', 'SHIRINTAGAB_11-1.R00-1A', 'BALKH_12-0.000-1M', 'BALKH_12-0.000-9M', 'BALKH_12-0.000-10M', 'BALKH_12-1.R00-1A', 'KHULM_13-0.000-1M', 'KHULM_13-0.000-2M', 'KUNDUZ_14-1.1L0-1A', 'KOKCHA_15-0.000-1M', 'KOKCHA_15-0.000-3M', 'KOKCHA_15-0.000-6M', 'KOKCHA_15-1.L00-1A', 'KOKCHA_15-10.R00-1A', 'KOKCHA_15-10.R00-2A', 'KUNDUZ_14-0.000-1M', 'KUNDUZ_14-0.000-2M', 'KUNDUZ_14-0.000-3M', 'KUNDUZ_14-0.000-4M', 'KUNDUZ_14-0.000-5S', 'KUNDUZ_14-0.000-6M', 'KUNDUZ_14-0.000-8M', 'KUNDUZ_14-1.R00-2A', 'KUNDUZ_14-1.R00-5A', 'KUNDUZ_14-5.R00-1A', 'KUNDUZ_14-9.5R0-1T', 'KUNDUZ_14-9.R00-1A', 'KUNDUZ_14-9.R00-6T', 'KUNDUZ_14-9.R00-8A', 'VAKSH_17089', 'VAKSH_17102', 'VAKSH_17106', 'VAKSH_17107', 'VAKSH_17109', 'VAKSH_17111', 'VAKSH_17113', 'VAKSH_17114', 'VAKSH_17115', 'VAKSH_17116', 'VAKSH_17119', 'VAKSH_17121', 'VAKSH_17122', 'KOFARNIKHAN_17125', 'KOFARNIKHAN_17126', 'KOFARNIKHAN_17141', 'KOFARNIKHAN_17147', 'KOFARNIKHAN_17157', 'KOFARNIKHAN_17160', 'KOFARNIKHAN_17165', 'KOFARNIKHAN_17169', 'KOFARNIKHAN_17170', 'KOFARNIKHAN_17172', 'KOFARNIKHAN_17176', 'KOFARNIKHAN_17177', 'KOFARNIKHAN_17178', 'KOFARNIKHAN_17179', 'ZERAFSHAN_17324', 'ZERAFSHAN_17333', 'ZERAFSHAN_17335', 'ZERAFSHAN_17341', 'ZERAFSHAN_17342', 'ZERAFSHAN_17343', 'ZERAFSHAN_70003', 'PYANDZH_17052', 'PYANDZH_17053', 'PYANDZH_17057', 'PYANDZH_17061', 'PYANDZH_17063', 'PYANDZH_17064', 'PYANDZH_17065', 'PYANDZH_17067', 'PYANDZH_17068', 'PYANDZH_17072', 'PYANDZH_17074', 'PYANDZH_17077', 'PYANDZH_17081', 'PYANDZH_70001', 'PYANDZH_70002', 'CHU_15102', 'CHU_15149', 'CHU_15171', 'CHU_15189', 'CHU_15194', 'CHU_15212', 'CHU_15214', 'CHU_15215', 'CHU_15216', 'TALAS_15256', 'TALAS_15259', 'TALAS_15261', 'TALAS_15278', 'TALAS_15283', 'TALAS_15285', 'TALAS_15287', 'TALAS_15290', 'TALAS_15292', 'TALAS_15312', 'SYR_DARYA_16133', 'NARYN_16936', 'ARYS_16326', 'ARYS_16340', 'ARYS_16363', 'ARYS_16374', 'ARYS_16375', 'ARYS_16390', 'NARYN_16055', 'NARYN_16059', 'NARYN_16068', 'NARYN_16070', 'NARYN_16076', 'SYR_DARYA_16101', 'SYR_DARYA_16121', 'SYR_DARYA_16127', 'SYR_DARYA_16134', 'SYR_DARYA_16135', 'SYR_DARYA_16136', 'SYR_DARYA_16139', 'SYR_DARYA_16143', 'SYR_DARYA_16146', 'SYR_DARYA_16151', 'SYR_DARYA_16153', 'SYR_DARYA_16158', 'SYR_DARYA_16159', 'SYR_DARYA_16169', 'SYR_DARYA_16176', 'SYR_DARYA_16487', 'SYR_DARYA_16510', 'SYR_DARYA_16107', 'CHIRCHIK_16262', 'CHIRCHIK_16279', 'CHIRCHIK_16290', 'CHIRCHIK_16298', 'CHIRCHIK_16300', 'CHIRCHIK_16924', 'SYR_DARYA_16938', 'CHIRCHIK_60048', 'CHIRCHIK_60032', 'CHIRCHIK_60033', 'CHIRCHIK_60034', 'CHIRCHIK_60035', 'CHIRCHIK_60036', 'CHIRCHIK_60037', 'CHIRCHIK_60038', 'CHIRCHIK_60039', 'CHIRCHIK_60040', 'CHIRCHIK_60041', 'AKHANGARAN_60042', 'AKHANGARAN_60043', 'AKHANGARAN_60044', 'AKHANGARAN_60045', 'AKHANGARAN_60046', 'AKHANGARAN_16230', 'NARYN_16057', 'NARYN_16072', 'NARYN_16074', 'NARYN_16075', 'NARYN_16080', 'NARYN_16081', 'NARYN_16082', 'NARYN_16085', 'NARYN_16087', 'NARYN_16088', 'NARYN_16093', 'NARYN_16096', 'NARYN_16100', 'NARYN_16103', 'NARYN_16105', 'SYR_DARYA_16124', 'SYR_DARYA_16137', 'SYR_DARYA_16154', 'SYR_DARYA_16155', 'SYR_DARYA_16160', 'SYR_DARYA_16161', 'SYR_DARYA_16162', 'SYR_DARYA_16163', 'SYR_DARYA_16164', 'SYR_DARYA_16175', 'SYR_DARYA_16187', 'SYR_DARYA_16192', 'SYR_DARYA_16193', 'SYR_DARYA_16198', 'SYR_DARYA_16202', 'SYR_DARYA_16205', 'SYR_DARYA_16210', 'SYR_DARYA_16211', 'NARYN_60002', 'NARYN_60003', 'NARYN_60005', 'NARYN_60006', 'NARYN_60007', 'NARYN_60008', 'NARYN_60009', 'NARYN_60010', 'NARYN_60011', 'NARYN_60012', 'NARYN_60013', 'NARYN_60014', 'NARYN_60015', 'NARYN_60016', 'NARYN_60017', 'NARYN_60018', 'NARYN_60019', 'NARYN_60020', 'NARYN_60021', 'SYR_DARYA_60022', 'SYR_DARYA_60023', 'SYR_DARYA_60024', 'SYR_DARYA_60025', 'SYR_DARYA_60026', 'SYR_DARYA_60029', 'SYR_DARYA_60031', 'MURGHAB_9-0.000-1M', 'MURGHAB_9-0.000-5M', 'MURGHAB_9-1.000-1A', 'MURGHAB_9-2.000-1A', 'MURGHAB_9-3.000-1A', 'MURGHAB_9-4.R00-8A', 'MURGHAB_9-5.L00-1A', 'HARIRUD_8-0.000-1M', 'HARIRUD_8-0.000-2M', 'HARIRUD_8-0.000-3S', 'HARIRUD_8-0.000-4M', 'HARIRUD_8-0.000-5M', 'HARIRUD_8-0.000-7M', 'HARIRUD_8-0.000-9M', 'HARIRUD_8-1.R00-9T', 'HARIRUD_8-3.L00-1A', 'HARIRUD_8-3.L00-6A', 'HARIRUD_8-11.L00-1A', 'HARIRUD_8-2.R00-3A']\n",
      "Processing catchment SYR_DARYA_16938...\n",
      "  Year: 2022\n",
      "Export started\n",
      "  Year: 2023\n",
      "Export started\n",
      "  Year: 2024\n",
      "Export started\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "##todo: automatically identify the last export date and continue from there\n",
    "\n",
    "# Loop over each basin and export results for a given month (or several months)\n",
    "year_first=2001\n",
    "year_last=2024\n",
    "catchment_names=RiverBasins_2023.aggregate_array('NAME').getInfo()\n",
    "print('catchment_names:', catchment_names)\n",
    "# #get the current month\n",
    "# current_month = datetime.datetime.now().month  # returns 1-12\n",
    "\n",
    "# Glacier mask: composite of all available glacier outlines saved as asset by tile (exported using the script create_glacier_mask_tiles.py)\n",
    "glacier_mask = ee.ImageCollection('projects/ee-hydro4u/assets/snow_CentralAsia/glacier_mask_collection').max()\n",
    "\n",
    "\n",
    "for catchment_name in catchment_names:\n",
    "    # filter by code 16230\n",
    "    aoi = RiverBasins_2023.filter(ee.Filter.eq('NAME', catchment_name)).geometry()\n",
    "\n",
    "    print(f\"Processing catchment {catchment_name}...\")\n",
    "\n",
    "    for year in range(year_first, year_last+1):\n",
    "        print(f\"  Year: {year}\")\n",
    "\n",
    "        # Load MODIS data for the year\n",
    "        modis_ic = create_decadal_composites_250(aoi, year, year, agg_interval=10)\n",
    "        # modis_ic = create_decadal_composites_250(aoi, 2025, 2025, agg_interval=10)\n",
    "\n",
    "        # Filter by date\n",
    "        modis_ic = ee.ImageCollection(modis_ic).filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "        # modis_ic = ee.ImageCollection(modis_ic).filter(ee.Filter.And(ee.Filter.calendarRange(2025, 2025, 'year'), ee.Filter.calendarRange(9,9, 'month')))\n",
    "           \n",
    "        # Function to process each image and create a feature with properties\n",
    "        def create_feature_with_properties(img):\n",
    "\n",
    "            # Calculate mean NIR value\n",
    "            mean_NIR_value = img.select('value').updateMask(glacier_mask).reduceRegion(\n",
    "                reducer=ee.Reducer.mean(),\n",
    "                geometry=aoi,\n",
    "                scale=250,\n",
    "                maxPixels=1e13,\n",
    "                tileScale=1\n",
    "            ).values()\n",
    "\n",
    "            # mark by -9999 if mean_NIR_value is null\n",
    "            mean_NIR_value = ee.Number(ee.Algorithms.If(mean_NIR_value.size().eq(0), -9999, mean_NIR_value.get(0)))\n",
    "\n",
    "            # Get date info\n",
    "            img_date = ee.Date(img.get('system:time_start'))\n",
    "            # img_year = img_date.get('year')\n",
    "            img_decade = ee.Number(img_date.get('day')).add(2).divide(10).ceil()\n",
    "            \n",
    "            # Create feature with all properties\n",
    "            feature = ee.Feature(None).set(\n",
    "                'Year-Month-Day', img_date.format('YYYY-MM-dd'),\n",
    "                'year', year,\n",
    "                'decade', img_decade,\n",
    "                'mean_NIR', mean_NIR_value,\n",
    "                'cc_fraction', img.get('cc_fraction'),\n",
    "                'cc_fraction2', img.get('cc_fraction2')\n",
    "            )\n",
    "\n",
    "            return feature\n",
    "        \n",
    "        # Apply the function to each image in the collection\n",
    "        aoi_mean_tmp = modis_ic.map(create_feature_with_properties)\n",
    "        \n",
    "        # print('aoi_mean_tmp:', aoi_mean_tmp.first().getInfo())\n",
    "        # aoi_mean_tmp: {'type': 'Feature', 'geometry': None, 'id': '15', 'properties': {'Year-Month-Day': '2024-06-01', 'decade': 1, 'mean_NIR': 0.5225649920662337, 'year': 2024}}\n",
    "\n",
    "        # Add geometry to features (because null geometry can't be exported)\n",
    "        joined = aoi_mean_tmp.map(lambda ft: ee.Feature(aoi.centroid(1000)).copyProperties(ft))\n",
    "        \n",
    "        # Sort by glacier snow cover below snowline\n",
    "        table_to_export = joined#.sort('gla_fsc_below_sl50', False)\n",
    "        \n",
    "        export_layer_name = 'decadal_meanNIR'  # Modify as needed\n",
    "        \n",
    "        # # Create year_month string for asset naming\n",
    "        # year_month = f\"{year}_{current_month-1:02d}\"\n",
    "        \n",
    "        # Export to asset\n",
    "        task = ee.batch.Export.table.toAsset(\n",
    "            collection=ee.FeatureCollection(table_to_export).set('NAME', catchment_name).set('YEAR', year),\n",
    "            description=f\"{export_layer_name}_{catchment_name.replace('.', '')}_Year_{year}\",\n",
    "            assetId=f\"projects/ee-hydro4u/assets/snow_CentralAsia/Folder4meanNIR/{export_layer_name}_{catchment_name.replace('.', '')}_Year_{year}_TEST2\"\n",
    "        )\n",
    "        \n",
    "        # Start the export task\n",
    "        task.start()\n",
    "        print(f\"Export started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check on new assets in the cloud project and share publicly to make it available in the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assets updated in the last 10 days: 0\n",
      "Oldest asset ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_AKHANGARAN_16230_until2025-01-01\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Once exports are completed, share all assets publically\n",
    "assetdir = f\"projects/ee-hydro4u/assets/{'snow_CentralAsia/Folder4SLA_v4'}\"#\n",
    "assets = ee.data.listAssets({\"parent\": assetdir})[\"assets\"]\n",
    "\n",
    "# Get the current UTC time and subtract 10 days\n",
    "cutoff_time = datetime.now(timezone.utc) - timedelta(days=10)\n",
    "\n",
    "# Filter assets updated in the last 10 days\n",
    "recent_assets = [\n",
    "    asset for asset in assets\n",
    "    if 'updateTime' in asset and datetime.fromisoformat(asset['updateTime'].replace('Z', '+00:00')) > cutoff_time\n",
    "]\n",
    "\n",
    "print(f\"Assets updated in the last 10 days: {len(recent_assets)}\")\n",
    "for table in recent_assets:\n",
    "    id = table['id']\n",
    "    asset = ee.data.getAsset(id)\n",
    "    ee.data.setIamPolicy(id, {\n",
    "        'bindings': [{'role': 'roles/owner', 'members': ['user:workshop.gee@gmail.com']},\n",
    "                        {'role': 'roles/viewer', 'members': ['allUsers']}]})\n",
    "    print('ID: ' + id)\n",
    "\n",
    "#get the oldest asset: sort by updateTime\n",
    "oldest_asset = sorted(assets, key=lambda x: x['updateTime'])[0]\n",
    "print('Oldest asset ID:', oldest_asset['id'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gap filling of exported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the exported data: data/fsc_sla_timeseries.csv\n",
    "# ---------------------------------------------\n",
    "# Load the exported data from Google Earth Engine\n",
    "# ---------------------------------------------\n",
    "# Define the path to the exported CSV file\n",
    "exported_csv_path = '../data/fsc_sla_timeseries.csv'\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(exported_csv_path)\n",
    "\n",
    "# Convert the 'Year-Month-Day' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['Year-Month-Day'])\n",
    "# Sort the DataFrame by 'Name' and then by 'date'\n",
    "df = df.sort_values(by=['Name', 'date'])\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['Name', 'date'], keep='last')\n",
    "# Calculate the time-steps between data points of the same catchment\n",
    "df['time_step'] = df.groupby('Name')['date'].diff().dt.days\n",
    "# Fill NaN values in the 'time_step' column with 0\n",
    "df['time_step'] = df['time_step'].fillna(0)\n",
    "\n",
    "# drop the 'system:index' column\n",
    "df = df.drop(columns=['system:index'])\n",
    "# drop the 'Year-Month-Day' column\n",
    "df = df.drop(columns=['Year-Month-Day'])\n",
    "\n",
    "# Define the decades in a month (1st, 2nd, and 3rd decade)\n",
    "decades = [1, 2, 3]\n",
    "results = []\n",
    "\n",
    "def safe_interpolate(series):\n",
    "    \"\"\"\n",
    "    Interpolates a series only if both previous and next values are available.\n",
    "    \"\"\"\n",
    "    s = series.copy()\n",
    "    for i in range(1, len(s) - 1):\n",
    "        if pd.isna(s.iloc[i]) and not pd.isna(s.iloc[i - 1]) and not pd.isna(s.iloc[i + 1]):\n",
    "            s.iloc[i] = (s.iloc[i - 1] + s.iloc[i + 1]) / 2\n",
    "    return s\n",
    "\n",
    "\n",
    "# Loop through each unique catchment name\n",
    "for catchment_name in df['Name'].unique():\n",
    "    catchment_data = df[df['Name'] == catchment_name].copy()  #  copy is essential\n",
    "\n",
    "    # Extract date parts safely\n",
    "    catchment_data['year'] = catchment_data['date'].dt.year\n",
    "    catchment_data['month'] = catchment_data['date'].dt.month\n",
    "\n",
    "    # Define decades\n",
    "    decades = [1, 2, 3]\n",
    "\n",
    "    # Get year range\n",
    "    years = range(catchment_data['year'].min(), 2024 + 1)\n",
    "    months = range(1, 13)\n",
    "\n",
    "    # Build full time grid\n",
    "    combinations = pd.DataFrame([\n",
    "        (year, month, decade) for year in years for month in months for decade in decades\n",
    "    ], columns=['year', 'month', 'decade'])\n",
    "\n",
    "    # Merge with actual data to find missing\n",
    "    merged_df = pd.merge(combinations, catchment_data, on=['year', 'month', 'decade'], how='left')\n",
    "\n",
    "    # Preserve catchment name\n",
    "    merged_df['Name'] = catchment_name\n",
    "    # Define a mapping for which day each decade starts on\n",
    "    decade_start_days = {1: 1, 2: 11, 3: 21}\n",
    "\n",
    "    # Apply to your merged_df to create a valid date\n",
    "    merged_df['date'] = pd.to_datetime({\n",
    "        'year': merged_df['year'],\n",
    "        'month': merged_df['month'],\n",
    "        'day': merged_df['decade'].map(decade_start_days)\n",
    "    })\n",
    "\n",
    "    # Fill NaN values in 'Basin' and 'Code' and '.geo' columns\n",
    "    merged_df['Basin'] = merged_df['Basin'].fillna(method='ffill')\n",
    "    merged_df['Code'] = merged_df['Code'].fillna(method='ffill')\n",
    "    merged_df['.geo'] = merged_df['.geo'].fillna(method='ffill')\n",
    "    # Fill NaN values in 'SLA_East', 'SLA_North', 'SLA_South', 'SLA_West', 'fsc', 'gla_area_below_sl50', and 'gla_fsc' columns\n",
    "    # interpolate between previous and next values\n",
    "    merged_df['SLA_East'] = merged_df['SLA_East'].interpolate(method='linear')\n",
    "    merged_df['SLA_North'] = merged_df['SLA_North'].interpolate(method='linear')\n",
    "    merged_df['SLA_South'] = merged_df['SLA_South'].interpolate(method='linear')\n",
    "    merged_df['SLA_West'] = merged_df['SLA_West'].interpolate(method='linear')\n",
    "    merged_df['fsc'] = merged_df['fsc'].interpolate(method='linear')\n",
    "    merged_df['gla_area_below_sl50'] = merged_df['gla_area_below_sl50'].interpolate(method='linear')\n",
    "    merged_df['gla_fsc'] = merged_df['gla_fsc'].interpolate(method='linear')\n",
    "\n",
    "    # 'gla_fsc_below_sl50' is 1 whenever 'gla_area_below_sl50' is 0\n",
    "    merged_df['gla_fsc_below_sl50'] = merged_df['gla_fsc_below_sl50'].mask(\n",
    "        merged_df['gla_area_below_sl50'] == 0, 1\n",
    "    )\n",
    "    merged_df['gla_fsc_below_sl50'] = merged_df['gla_fsc_below_sl50'].interpolate(method='linear')\n",
    "\n",
    "    # Identify rows where gla_fsc is NaN\n",
    "    na_mask = merged_df['gla_fsc'].isna()\n",
    "\n",
    "    # Set both columns to 0 where gla_fsc is NaN\n",
    "    merged_df.loc[na_mask, 'gla_area_below_sl50'] = 0\n",
    "    merged_df.loc[na_mask, 'gla_fsc_below_sl50'] = 0\n",
    "    merged_df.loc[na_mask, 'gla_fsc'] = 0\n",
    "\n",
    "    # for col in [\n",
    "    #     'SLA_East', 'SLA_North', 'SLA_South', 'SLA_West',\n",
    "    #     'fsc', 'gla_area_below_sl50', 'gla_fsc', 'gla_fsc_below_sl50'\n",
    "    # ]:\n",
    "    #     merged_df[col] = safe_interpolate(merged_df[col])\n",
    "\n",
    "    results.append(merged_df)\n",
    "\n",
    "# Concatenate all catchments into one full DataFrame\n",
    "df_full = pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# print(df_full)\n",
    "\n",
    "df = df_full.copy()\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "# drop the 'time_step' column\n",
    "df = df.drop(columns=['time_step'])\n",
    "\n",
    "# Plotting the data\n",
    "# ---------------------------------------------\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Loop through each unique catchment name\n",
    "for catchment_name in df['Name'].unique():\n",
    "    # Filter the DataFrame for the current catchment\n",
    "    catchment_data = df[df['Name'] == catchment_name]\n",
    "    # Plot the data for the current catchment\n",
    "    plt.plot(catchment_data['date'], catchment_data['SLA_East'], label=catchment_name)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Snowline Elevation by Aspect')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Snowline Elevation (m)')\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "# Add a legend\n",
    "# plt.legend()\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the maximum snowline elevation for each catchment\n",
    "for catchment_name in df['Name'].unique():\n",
    "    catchment_data = df[df['Name'] == catchment_name]\n",
    "    max_snowline = catchment_data['SLA_North'].max()\n",
    "    print(f\"Maximum snowline elevation for {catchment_name}: {max_snowline} m\")\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = '../data/fsc_sla_timeseries_gapfilled.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Data saved to {output_csv_path}\")\n",
    "# ---------------------------------------------\n",
    "# End of script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
