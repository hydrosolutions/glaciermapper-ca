{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GlacierMapper-CA: Snow Cover Analysis Notebook\n",
    "\n",
    "This notebook provides comprehensive analysis tools for snow cover mapping in Central Asian glacierized basins using MODIS satellite data and digital elevation models.\n",
    "\n",
    "## Overview\n",
    "- **MODIS Data Processing**: Load and process 500m and 250m resolution data\n",
    "- **Snowline Detection**: Automated snowline altitude estimation by terrain aspect\n",
    "- **Glacier Monitoring**: Extract snow metrics over glacierized areas\n",
    "- **Time Series Analysis**: Generate decadal composites and gap-filled datasets\n",
    "- **Data Export**: Batch processing for multiple basins with cloud asset management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Setting up the Python environment and project paths for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Verify the environment is working\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path includes project root: {str(project_root) in sys.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Earth Engine Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "# # Authenticate and initialize Earth Engine\n",
    "# ee.Authenticate(auth_mode='notebook', force=True)\n",
    "# ee.Initialize(project=\"ee-sahellakes\")\n",
    "ee.Initialize(project=\"thurgau-irrigation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Library Imports and Data Loading\n",
    "\n",
    "Import required libraries and load base datasets including river basins and glacier inventories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snow Cover Mapping Notebook\n",
    "\n",
    "# This cell initializes Earth Engine and sets up paths and config\n",
    "\n",
    "import geemap\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import your project-specific modules from `src`\n",
    "from src.modis_processing import (load_modis, fill_modis_with_aqua, add_date_bands,create_decadal_composites, extract_year_ranges,process_interval,process_interval_250,\n",
    "    load_modis_250,fill_modis_with_aqua_250, create_decadal_composites_250,modis_cloud_masking_250\n",
    ")\n",
    "from src.dem_processing import load_dem, classify_aspect, reproject_and_analyze_dem\n",
    "from src.snowline import get_snowline_elevation, calculate_glacier_metrics\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load AOI from RiverBasins_2023 asset\n",
    "# ---------------------------------------------\n",
    "RiverBasins_2023 = ee.FeatureCollection('users/hydrosolutions/RiverBasins_CA_Jan2023_simple1000')\n",
    "RiverBasins_2023 = RiverBasins_2023.map(lambda ft: ft.set('NAME', ee.String(ft.get('BASIN')).cat(ee.String('_')).cat(ee.String(ft.get('CODE')))))\n",
    "glims = ee.FeatureCollection(\"GLIMS/20230607\").filter(ee.Filter.eq('geog_area', \"Randolph Glacier Inventory; Umbrella RC for merging the RGI into GLIMS\"));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithm Testing & Development\n",
    "\n",
    "Development and testing area for MODIS processing algorithms, snowline detection, and glacier metrics calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filter by code 15002\n",
    "aoi = RiverBasins_2023.filter(ee.Filter.eq('CODE', '15002')).geometry()\n",
    "print('AOI:', aoi.getInfo())\n",
    "\n",
    "start_year = 2024\n",
    "end_year = 2024\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load and process MODIS\n",
    "# ---------------------------------------------\n",
    "intervals = ee.List(extract_year_ranges(ee.List.sequence(start_year, end_year), 10)\n",
    "    .iterate(lambda list, previous: ee.List(previous).cat(ee.List(list)), ee.List([])))\n",
    "\n",
    "# print('intervals:',intervals)\n",
    "print('interval:', ee.List(intervals.slice(19,20).get(0)).getInfo())\n",
    "\n",
    "# Load all MODIS data for the entire time span\n",
    "start_date = ee.Date.fromYMD(start_year, 1, 1)\n",
    "end_date = ee.Date.fromYMD(end_year, 12, 31)\n",
    "terra_coll = load_modis(aoi)\n",
    "\n",
    "# Create filled MODIS snow cover fraction collection\n",
    "mscf = terra_coll.map(lambda img: fill_modis_with_aqua(img))\n",
    "# print('mscf:', mscf.first().getInfo())\n",
    "\n",
    "# time_intervals_all should be an ee.List of [start, end] ee.Date pairs\n",
    "modis_ic = ee.ImageCollection(intervals.slice(19,20).map(lambda list:process_interval(mscf, list)))\n",
    "# print('modis_ic:', modis_ic.filterBounds(aoi).getInfo())\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load MODIS 250 m data (only for testing)\n",
    "# ---------------------------------------------\n",
    "terra_coll = load_modis_250(aoi)\n",
    "# mscf = terra_coll.map(lambda img: modis_cloud_masking_250(img,aoi))\n",
    "# print('mscf 250:', mscf.first().getInfo())\n",
    "# # time_intervals_all should be an ee.List of [start, end] ee.Date pairs\n",
    "# modis_ic_250 = ee.ImageCollection(intervals.slice(19,20).map(lambda list:process_interval_250(mscf, list)))\n",
    "# print('modis_ic 250:', modis_ic_250.filterBounds(aoi).getInfo())\n",
    "\n",
    "mscf = terra_coll.map(lambda img: fill_modis_with_aqua_250(img,aoi))\n",
    "print('mscf 250:', mscf.first().getInfo())\n",
    "# time_intervals_all should be an ee.List of [start, end] ee.Date pairs\n",
    "modis_ic_250 = ee.ImageCollection(intervals.slice(19,20).map(lambda list:process_interval_250(mscf, list)))\n",
    "print('modis_ic 250:', modis_ic_250.filterBounds(aoi).getInfo())\n",
    "\n",
    "# modis_ic = create_decadal_composites(aoi, start_year, end_year, agg_interval=10)\n",
    "# print('modis_ic:', modis_ic.filterBounds(aoi).first().getInfo())\n",
    "# terra, aqua = load_modis(aoi, start_date, end_date)\n",
    "# filled_modis = terra.map(lambda img: fill_modis_with_aqua(img, aqua))\n",
    "# filled_modis = filled_modis.map(add_date_bands)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load DEM and classify terrain aspect\n",
    "# ---------------------------------------------\n",
    "dem = load_dem()\n",
    "modisProjection = modis_ic.filterBounds(aoi).first().projection()\n",
    "scale = 500\n",
    "tile_scale = 2\n",
    "sc_th = 50\n",
    "aspect_keys = ['East', 'North', 'South', 'West', 'mixed']\n",
    "# Reproject and analyze DEM\n",
    "reprojected_dem, min_dem_dict, max_dem_dict, n_grid = reproject_and_analyze_dem(dem, modisProjection, aoi, scale, tile_scale, aspect_keys)\n",
    "# print('reprojected_dem:', reprojected_dem.getInfo())\n",
    "\n",
    "aspects, aspect_coded = classify_aspect(dem, modisProjection, scale)\n",
    "# print('aspects:', aspects.bandNames().getInfo())\n",
    "# print('n_grid:', n_grid.getInfo())\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Snowline elevation estimation from first MODIS image\n",
    "# ---------------------------------------------\n",
    "sample_img = ee.Image(modis_ic.filterDate(start_date, end_date).first())\n",
    "\n",
    "# Test on a problematic summer image\n",
    "sample_img = modis_ic.filterDate('2024-07-01', '2024-08-01').first()\n",
    "\n",
    "print('sample_img:', sample_img.getInfo())\n",
    "# Get snowline elevation and statistics\n",
    "snowline_stats,fsc = get_snowline_elevation(sample_img, reprojected_dem, aspect_coded, aoi,min_dem_dict, max_dem_dict,n_grid, \n",
    "                                                      scale=500, scale_dem=500, sc_th=50, canny_threshold=0.7, \n",
    "                           canny_sigma=0.7, ppha=10, tile_scale=1, point2sample=1000, aspectKeys=['East', 'North', 'South', 'West', 'mixed'])\n",
    "  \n",
    "print('Snowline elevation by aspect:', snowline_stats.getInfo())\n",
    "print('Fractional Snow Cover', fsc.getInfo())\n",
    "\n",
    "glacier_metrics=calculate_glacier_metrics(glims, aoi, sample_img,sc_th, snowline_stats , dem, aspect_keys, tile_scale, aspects)\n",
    "# Access individual EE values from the dictionary and get their info\n",
    "# print('Glacier SCF:', glacier_metrics['glims_fsc'].getInfo())\n",
    "# print('Glacier SCF below snowline:', glacier_metrics['glims_fsc_below_sl'].getInfo())\n",
    "# print('Glacier area below snowline:', glacier_metrics['glims_area_below_sl'].getInfo())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing: MODIS 500m Snow Cover Data\n",
    "\n",
    "Automated processing pipeline for generating snowline altitude (SLA) and fractional snow cover (FSC) time series across all Central Asian basins using MODIS 500m data.\n",
    "\n",
    "### Features:\n",
    "- **Multi-basin processing**: Loops through all catchments\n",
    "- **Current month detection**: Automatically processes up to previous month  \n",
    "- **Snowline detection**: Aspect-specific elevation estimates (East, North, South, West)\n",
    "- **Glacier metrics**: Snow cover statistics for glacierized areas\n",
    "- **Cloud export**: Direct upload to Google Earth Engine assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "##todo: automatically identify the last export date and continue from there\n",
    "\n",
    "# Loop over each basin and export results for a given month (or several months)\n",
    "year=2025\n",
    "catchment_names=RiverBasins_2023.aggregate_array('NAME').getInfo()\n",
    "#get the current month\n",
    "current_month = datetime.datetime.now().month  # returns 1-12\n",
    "\n",
    "for catchment_name in catchment_names:\n",
    "\n",
    "    print(f\"Processing catchment {catchment_name}...\")\n",
    "    \n",
    "    # filter by code 16230\n",
    "    aoi = RiverBasins_2023.filter(ee.Filter.eq('NAME', catchment_name)).geometry()\n",
    "\n",
    "    # Load MODIS data for the year\n",
    "    modis_ic = create_decadal_composites(aoi, year, year, agg_interval=10)\n",
    "\n",
    "    # Define start and end dates for the year: run from the beginning to the previous month from now\n",
    "    start_date = ee.Date.fromYMD(year, 1, 1)\n",
    "    end_date = ee.Date.fromYMD(year, current_month - 1, 31)\n",
    "    \n",
    "    # Filter by date\n",
    "    modis_ic = ee.ImageCollection(modis_ic).filter(ee.Filter.And(ee.Filter.calendarRange(year, year, 'year'), ee.Filter.calendarRange(1, current_month - 1, 'month')))\n",
    "    modisProjection = modis_ic.filterBounds(aoi).first().projection()\n",
    "    reprojected_dem, min_dem_dict, max_dem_dict, n_grid = reproject_and_analyze_dem(dem, modisProjection, aoi, scale, tile_scale, aspect_keys)\n",
    "    aspects, aspect_coded = classify_aspect(dem, modisProjection, scale)    \n",
    "    \n",
    "    # Function to process each image and create a feature with properties\n",
    "    def create_feature_with_properties(img):\n",
    "        # Get snowline elevation for this image\n",
    "        current_snowline_stats, current_fsc = get_snowline_elevation(\n",
    "            img, reprojected_dem, aspect_coded, aoi, min_dem_dict, max_dem_dict, n_grid,\n",
    "            scale=500, scale_dem=500, sc_th=50, canny_threshold=0.7,\n",
    "            canny_sigma=0.7, ppha=10, tile_scale=1, point2sample=1000, \n",
    "            aspectKeys=aspect_keys\n",
    "        )\n",
    "        \n",
    "        # Calculate glacier metrics\n",
    "        current_glacier_metrics = calculate_glacier_metrics(\n",
    "            glims, aoi, img, sc_th, current_snowline_stats, dem, aspect_keys, tile_scale, aspects\n",
    "        )\n",
    "        \n",
    "        # Create a dictionary with aspect-specific values\n",
    "        aspect_dict = {}\n",
    "        base = ee.String('SLA_')\n",
    "        \n",
    "        # Add properties for each aspect (excluding 'mixed')\n",
    "        for i, aspect in enumerate(aspect_keys[:-1]):\n",
    "            aspect_dict[base.cat(aspect)] = current_snowline_stats.get(aspect)\n",
    "        \n",
    "        # Get date info\n",
    "        img_date = ee.Date(img.get('system:time_start'))\n",
    "        img_year = img_date.get('year')\n",
    "        img_decade = ee.Number(img_date.get('day')).add(2).divide(10).ceil()\n",
    "        \n",
    "        # Create feature with all properties\n",
    "        feature = ee.Feature(None).set(\n",
    "            'Year-Month-Day', img_date.format('YYYY-MM-dd'),\n",
    "            'year', img_year,\n",
    "            'decade', img_decade,\n",
    "            'gla_fsc', current_glacier_metrics['glims_fsc'],\n",
    "            'gla_fsc_below_sl50', current_glacier_metrics['glims_fsc_below_sl'],\n",
    "            'gla_area_below_sl50', current_glacier_metrics['glims_area_below_sl'],\n",
    "            'fsc', current_fsc\n",
    "        )\n",
    "        \n",
    "        # Add aspect-specific properties\n",
    "        for key, value in aspect_dict.items():\n",
    "            feature = feature.set(key, value)\n",
    "            \n",
    "        return feature\n",
    "    \n",
    "    # Apply the function to each image in the collection\n",
    "    aoi_mean_tmp = modis_ic.map(create_feature_with_properties)\n",
    "    \n",
    "    # Add geometry to features (because null geometry can't be exported)\n",
    "    joined = aoi_mean_tmp.map(lambda ft: ee.Feature(aoi.centroid(1000)).copyProperties(ft))\n",
    "    \n",
    "    # Sort by glacier snow cover below snowline\n",
    "    table_to_export = joined#.sort('gla_fsc_below_sl50', False)\n",
    "    \n",
    "    export_layer_name = 'decadal_SLA'  # Modify as needed\n",
    "    \n",
    "    # Create year_month string for asset naming\n",
    "    year_month = f\"{year}_{current_month-1:02d}\"\n",
    "    \n",
    "    # Export to asset\n",
    "    task = ee.batch.Export.table.toAsset(\n",
    "        collection=ee.FeatureCollection(table_to_export).set('NAME', catchment_name),\n",
    "        description=f\"{export_layer_name}_{catchment_name.replace('.', '')}_{year_month}\",\n",
    "        assetId=f\"projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/{export_layer_name}_{catchment_name.replace('.', '')}_{year_month}\"\n",
    "    )\n",
    "    \n",
    "    # # Start the export task\n",
    "    task.start()\n",
    "    print(f\"Export started for year {year} and month {current_month-1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Historical Data Processing (2001-2024)\n",
    "\n",
    "Alternative processing approach for generating annual historical datasets. This cell processes full year ranges for comprehensive time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through years 2001 to 2024 (annual assets)\n",
    "for year in range(start_year, end_year + 1):\n",
    "\n",
    "    print(f\"Processing year {year}...\")\n",
    "    \n",
    "    # Define start and end dates for the year\n",
    "    start_date = ee.Date.fromYMD(year, 1, 1)\n",
    "    end_date = ee.Date.fromYMD(year, 12, 31)\n",
    "    \n",
    "    # Load MODIS data for the year\n",
    "    modis_ic = create_decadal_composites(aoi, year, year, agg_interval=10)\n",
    "    \n",
    "    # Load and process DEM if needed (or reuse from outside the loop if not changing)\n",
    "    dem = load_dem()\n",
    "    modisProjection = modis_ic.filterBounds(aoi).first().projection()\n",
    "    \n",
    "    # Function to process each image and create a feature with properties\n",
    "    def create_feature_with_properties(img):\n",
    "        # Get snowline elevation for this image\n",
    "        current_snowline_stats, current_fsc = get_snowline_elevation(\n",
    "            img, reprojected_dem, aspect_coded, aoi, min_dem_dict, max_dem_dict, n_grid,\n",
    "            scale=500, scale_dem=500, sc_th=50, canny_threshold=0.7,\n",
    "            canny_sigma=0.7, ppha=10, tile_scale=1, point2sample=1000, \n",
    "            aspectKeys=aspect_keys\n",
    "        )\n",
    "        \n",
    "        # Calculate glacier metrics\n",
    "        current_glacier_metrics = calculate_glacier_metrics(\n",
    "            glims, aoi, img, sc_th, current_snowline_stats, dem, aspect_keys, tile_scale, aspects\n",
    "        )\n",
    "        \n",
    "        # Create a dictionary with aspect-specific values\n",
    "        aspect_dict = {}\n",
    "        base = ee.String('SLA_')\n",
    "        \n",
    "        # Add properties for each aspect (excluding 'mixed')\n",
    "        for i, aspect in enumerate(aspect_keys[:-1]):\n",
    "            aspect_dict[base.cat(aspect)] = current_snowline_stats.get(aspect)\n",
    "        \n",
    "        # Get date info\n",
    "        img_date = ee.Date(img.get('system:time_start'))\n",
    "        img_year = img_date.get('year')\n",
    "        img_decade = ee.Number(img_date.get('day')).add(2).divide(10).ceil()\n",
    "        \n",
    "        # Create feature with all properties\n",
    "        feature = ee.Feature(None).set(\n",
    "            'Year-Month-Day', img_date.format('YYYY-MM-dd'),\n",
    "            'year', img_year,\n",
    "            'decade', img_decade,\n",
    "            'gla_fsc', current_glacier_metrics['glims_fsc'],\n",
    "            'gla_fsc_below_sl50', current_glacier_metrics['glims_fsc_below_sl'],\n",
    "            'gla_area_below_sl50', current_glacier_metrics['glims_area_below_sl'],\n",
    "            'fsc', current_fsc\n",
    "        )\n",
    "        \n",
    "        # Add aspect-specific properties\n",
    "        for key, value in aspect_dict.items():\n",
    "            feature = feature.set(key, value)\n",
    "            \n",
    "        return feature\n",
    "    \n",
    "    # Apply the function to each image in the collection\n",
    "    aoi_mean_tmp = modis_ic.map(create_feature_with_properties)\n",
    "    \n",
    "    # Add geometry to features (because null geometry can't be exported)\n",
    "    joined = aoi_mean_tmp.map(lambda ft: ee.Feature(aoi.centroid(1000)).copyProperties(ft))\n",
    "    \n",
    "    # Sort by glacier snow cover below snowline\n",
    "    table_to_export = joined#.sort('gla_fsc_below_sl50', False)\n",
    "    \n",
    "    # Set catchment name (assuming it's defined elsewhere)\n",
    "    catchment_name = 'AKHANGARAN_16230'  # Modify as needed\n",
    "    export_layer_name = 'decadal_SLA'  # Modify as needed\n",
    "    \n",
    "    # Export to asset\n",
    "    task = ee.batch.Export.table.toAsset(\n",
    "        collection=ee.FeatureCollection(table_to_export).set('NAME', catchment_name),\n",
    "        description=f\"{export_layer_name}_{catchment_name.replace('.', '')}_Year{year}\",\n",
    "        assetId=f\"projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/{export_layer_name}_{catchment_name.replace('.', '')}_Year{year}\"\n",
    "    )\n",
    "    \n",
    "    # Start the export task\n",
    "    task.start()\n",
    "    print(f\"Export started for year {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MODIS 250m Reflectance Processing\n",
    "\n",
    "Processing pipeline for extracting Near-Infrared (NIR) reflectance time series from MODIS 250m data over glacierized areas.\n",
    "\n",
    "### Purpose:\n",
    "- **High-resolution monitoring**: 250m spatial resolution for detailed glacier analysis\n",
    "- **NIR reflectance**: Sensitive indicator of glacier surface conditions and snow cover (Kim et al., 2025)\n",
    "- **Glacier masking**: Uses composite glacier mask from all available outlines\n",
    "- **Multi-year processing**: Generates decadal time series from 2001-2024\n",
    "\n",
    "### Reference:\n",
    "Kim, D., Mattea, E., Callegari, M., Saks, T., Kenzhebayev, R., Azisov, E., Ullmann, T., Hoelzle, M., and Barandun, M.: Sub-seasonal snowline dynamics of glaciers in Central Asia from multi-sensor satellite observations, 2000–2023, EGUsphere [preprint], https://doi.org/10.5194/egusphere-2025-3978, 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check position of catchment in the list\n",
    "catchment_names=RiverBasins_2023.aggregate_array('NAME').getInfo()\n",
    "print('catchment_names:', catchment_names)\n",
    "#position of the catchment with id 16205 in NAME\n",
    "position=catchment_names.index('SYR_DARYA_16938')\n",
    "print('position:', position)\n",
    "position=catchment_names.index('SYR_DARYA_60024')\n",
    "print('position:', position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "##todo: automatically identify the last export date and continue from there\n",
    "\n",
    "# Loop over each basin and export results for a given month (or several months)\n",
    "year_first=2001\n",
    "year_last=2024\n",
    "catchment_names=RiverBasins_2023.aggregate_array('NAME').getInfo()\n",
    "print('catchment_names:', catchment_names)\n",
    "# #get the current month\n",
    "# current_month = datetime.datetime.now().month  # returns 1-12\n",
    "\n",
    "# Glacier mask: composite of all available glacier outlines saved as asset by tile (exported using the script create_glacier_mask_tiles.py)\n",
    "glacier_mask = ee.ImageCollection('projects/ee-hydro4u/assets/snow_CentralAsia/glacier_mask_collection').max()\n",
    "\n",
    "\n",
    "for catchment_name in catchment_names:\n",
    "    # filter by code 16230\n",
    "    aoi = RiverBasins_2023.filter(ee.Filter.eq('NAME', catchment_name)).geometry()\n",
    "\n",
    "    print(f\"Processing catchment {catchment_name}...\")\n",
    "\n",
    "    for year in range(year_first, year_last+1):\n",
    "        print(f\"  Year: {year}\")\n",
    "\n",
    "        # Load MODIS data for the year\n",
    "        modis_ic = create_decadal_composites_250(aoi, year, year, agg_interval=10)\n",
    "        # modis_ic = create_decadal_composites_250(aoi, 2025, 2025, agg_interval=10)\n",
    "\n",
    "        # Filter by date\n",
    "        modis_ic = ee.ImageCollection(modis_ic).filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "        # modis_ic = ee.ImageCollection(modis_ic).filter(ee.Filter.And(ee.Filter.calendarRange(2025, 2025, 'year'), ee.Filter.calendarRange(9,9, 'month')))\n",
    "           \n",
    "        # Function to process each image and create a feature with properties\n",
    "        def create_feature_with_properties(img):\n",
    "\n",
    "            # Calculate mean NIR value\n",
    "            mean_NIR_value = img.select('value').updateMask(glacier_mask).reduceRegion(\n",
    "                reducer=ee.Reducer.mean(),\n",
    "                geometry=aoi,\n",
    "                scale=250,\n",
    "                maxPixels=1e13,\n",
    "                tileScale=1\n",
    "            ).values()\n",
    "\n",
    "            # mark by -9999 if mean_NIR_value is null\n",
    "            mean_NIR_value = ee.Number(ee.Algorithms.If(mean_NIR_value.size().eq(0), -9999, mean_NIR_value.get(0)))\n",
    "\n",
    "            # Get date info\n",
    "            img_date = ee.Date(img.get('system:time_start'))\n",
    "            # img_year = img_date.get('year')\n",
    "            img_decade = ee.Number(img_date.get('day')).add(2).divide(10).ceil()\n",
    "            \n",
    "            # Create feature with all properties\n",
    "            feature = ee.Feature(None).set(\n",
    "                'Year-Month-Day', img_date.format('YYYY-MM-dd'),\n",
    "                'year', year,\n",
    "                'decade', img_decade,\n",
    "                'mean_NIR', mean_NIR_value,\n",
    "                'cc_fraction', img.get('cc_fraction'),\n",
    "                'cc_fraction2', img.get('cc_fraction2')\n",
    "            )\n",
    "\n",
    "            return feature\n",
    "        \n",
    "        # Apply the function to each image in the collection\n",
    "        aoi_mean_tmp = modis_ic.map(create_feature_with_properties)\n",
    "        \n",
    "        # print('aoi_mean_tmp:', aoi_mean_tmp.first().getInfo())\n",
    "        # aoi_mean_tmp: {'type': 'Feature', 'geometry': None, 'id': '15', 'properties': {'Year-Month-Day': '2024-06-01', 'decade': 1, 'mean_NIR': 0.5225649920662337, 'year': 2024}}\n",
    "\n",
    "        # Add geometry to features (because null geometry can't be exported)\n",
    "        joined = aoi_mean_tmp.map(lambda ft: ee.Feature(aoi.centroid(1000)).copyProperties(ft))\n",
    "        \n",
    "        # Sort by glacier snow cover below snowline\n",
    "        table_to_export = joined#.sort('gla_fsc_below_sl50', False)\n",
    "        \n",
    "        export_layer_name = 'decadal_meanNIR'  # Modify as needed\n",
    "        \n",
    "        # # Create year_month string for asset naming\n",
    "        # year_month = f\"{year}_{current_month-1:02d}\"\n",
    "        \n",
    "        # Export to asset\n",
    "        task = ee.batch.Export.table.toAsset(\n",
    "            collection=ee.FeatureCollection(table_to_export).set('NAME', catchment_name).set('YEAR', year),\n",
    "            description=f\"{export_layer_name}_{catchment_name.replace('.', '')}_Year_{year}\",\n",
    "            assetId=f\"projects/ee-hydro4u/assets/snow_CentralAsia/Folder4meanNIR/{export_layer_name}_{catchment_name.replace('.', '')}_Year_{year}_TEST2\"\n",
    "        )\n",
    "        \n",
    "        # Start the export task\n",
    "        task.start()\n",
    "        print(f\"Export started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Asset Management & Public Sharing\n",
    "\n",
    "Utilities for managing Google Earth Engine assets and making them publicly accessible for the GlacierMapper-CA web application.\n",
    "\n",
    "### Functions:\n",
    "- **Recent asset detection**: Find assets updated in the last 10 days\n",
    "- **Public sharing**: Set appropriate permissions for web application access\n",
    "- **Asset inventory**: Track oldest and newest assets for maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Once exports are completed, share all assets publically\n",
    "assetdir = f\"projects/ee-hydro4u/assets/{'snow_CentralAsia/Folder4SLA_v4'}\"#\n",
    "assets = ee.data.listAssets({\"parent\": assetdir})[\"assets\"]\n",
    "\n",
    "# Get the current UTC time and subtract 10 days\n",
    "cutoff_time = datetime.now(timezone.utc) - timedelta(days=10)\n",
    "\n",
    "# Filter assets updated in the last 10 days\n",
    "recent_assets = [\n",
    "    asset for asset in assets\n",
    "    if 'updateTime' in asset and datetime.fromisoformat(asset['updateTime'].replace('Z', '+00:00')) > cutoff_time\n",
    "]\n",
    "\n",
    "print(f\"Assets updated in the last 10 days: {len(recent_assets)}\")\n",
    "for table in recent_assets:\n",
    "    id = table['id']\n",
    "    asset = ee.data.getAsset(id)\n",
    "    ee.data.setIamPolicy(id, {\n",
    "        'bindings': [{'role': 'roles/owner', 'members': ['user:workshop.gee@gmail.com']},\n",
    "                        {'role': 'roles/viewer', 'members': ['allUsers']}]})\n",
    "    print('ID: ' + id)\n",
    "\n",
    "#get the oldest asset: sort by updateTime\n",
    "oldest_asset = sorted(assets, key=lambda x: x['updateTime'])[0]\n",
    "print('Oldest asset ID:', oldest_asset['id'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Post-Processing & Gap Filling\n",
    "\n",
    "Post-processing pipeline for exported Google Earth Engine data to create complete, gap-filled time series datasets.\n",
    "\n",
    "### Processing Steps:\n",
    "- **Data loading**: Import exported CSV files from GEE\n",
    "- **Temporal alignment**: Create complete decadal time grid (2001-2024)\n",
    "- **Gap filling**: Linear interpolation for missing values\n",
    "- **Quality control**: Handle edge cases and data validation\n",
    "- **Visualization**: Generate time series plots for quality assessment\n",
    "- **Export**: Save gap-filled datasets for analysis and web application use\n",
    "\n",
    "### Output:\n",
    "- Complete time series with no temporal gaps\n",
    "- Standardized decadal temporal resolution\n",
    "- Ready-to-use datasets for the GlacierMapper-CA web application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the exported data: data/fsc_sla_timeseries.csv\n",
    "# ---------------------------------------------\n",
    "# Load the exported data from Google Earth Engine\n",
    "# ---------------------------------------------\n",
    "# Define the path to the exported CSV file\n",
    "exported_csv_path = '../data/fsc_sla_timeseries.csv'\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(exported_csv_path)\n",
    "\n",
    "# Convert the 'Year-Month-Day' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['Year-Month-Day'])\n",
    "# Sort the DataFrame by 'Name' and then by 'date'\n",
    "df = df.sort_values(by=['Name', 'date'])\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['Name', 'date'], keep='last')\n",
    "# Calculate the time-steps between data points of the same catchment\n",
    "df['time_step'] = df.groupby('Name')['date'].diff().dt.days\n",
    "# Fill NaN values in the 'time_step' column with 0\n",
    "df['time_step'] = df['time_step'].fillna(0)\n",
    "\n",
    "# drop the 'system:index' column\n",
    "df = df.drop(columns=['system:index'])\n",
    "# drop the 'Year-Month-Day' column\n",
    "df = df.drop(columns=['Year-Month-Day'])\n",
    "\n",
    "# Define the decades in a month (1st, 2nd, and 3rd decade)\n",
    "decades = [1, 2, 3]\n",
    "results = []\n",
    "\n",
    "def safe_interpolate(series):\n",
    "    \"\"\"\n",
    "    Interpolates a series only if both previous and next values are available.\n",
    "    \"\"\"\n",
    "    s = series.copy()\n",
    "    for i in range(1, len(s) - 1):\n",
    "        if pd.isna(s.iloc[i]) and not pd.isna(s.iloc[i - 1]) and not pd.isna(s.iloc[i + 1]):\n",
    "            s.iloc[i] = (s.iloc[i - 1] + s.iloc[i + 1]) / 2\n",
    "    return s\n",
    "\n",
    "\n",
    "# Loop through each unique catchment name\n",
    "for catchment_name in df['Name'].unique():\n",
    "    catchment_data = df[df['Name'] == catchment_name].copy()  # ← copy is essential\n",
    "\n",
    "    # Extract date parts safely\n",
    "    catchment_data['year'] = catchment_data['date'].dt.year\n",
    "    catchment_data['month'] = catchment_data['date'].dt.month\n",
    "\n",
    "    # Define decades\n",
    "    decades = [1, 2, 3]\n",
    "\n",
    "    # Get year range\n",
    "    years = range(catchment_data['year'].min(), 2024 + 1)\n",
    "    months = range(1, 13)\n",
    "\n",
    "    # Build full time grid\n",
    "    combinations = pd.DataFrame([\n",
    "        (year, month, decade) for year in years for month in months for decade in decades\n",
    "    ], columns=['year', 'month', 'decade'])\n",
    "\n",
    "    # Merge with actual data to find missing\n",
    "    merged_df = pd.merge(combinations, catchment_data, on=['year', 'month', 'decade'], how='left')\n",
    "\n",
    "    # Preserve catchment name\n",
    "    merged_df['Name'] = catchment_name\n",
    "    # Define a mapping for which day each decade starts on\n",
    "    decade_start_days = {1: 1, 2: 11, 3: 21}\n",
    "\n",
    "    # Apply to your merged_df to create a valid date\n",
    "    merged_df['date'] = pd.to_datetime({\n",
    "        'year': merged_df['year'],\n",
    "        'month': merged_df['month'],\n",
    "        'day': merged_df['decade'].map(decade_start_days)\n",
    "    })\n",
    "\n",
    "    # Fill NaN values in 'Basin' and 'Code' and '.geo' columns\n",
    "    merged_df['Basin'] = merged_df['Basin'].fillna(method='ffill')\n",
    "    merged_df['Code'] = merged_df['Code'].fillna(method='ffill')\n",
    "    merged_df['.geo'] = merged_df['.geo'].fillna(method='ffill')\n",
    "    # Fill NaN values in 'SLA_East', 'SLA_North', 'SLA_South', 'SLA_West', 'fsc', 'gla_area_below_sl50', and 'gla_fsc' columns\n",
    "    # interpolate between previous and next values\n",
    "    merged_df['SLA_East'] = merged_df['SLA_East'].interpolate(method='linear')\n",
    "    merged_df['SLA_North'] = merged_df['SLA_North'].interpolate(method='linear')\n",
    "    merged_df['SLA_South'] = merged_df['SLA_South'].interpolate(method='linear')\n",
    "    merged_df['SLA_West'] = merged_df['SLA_West'].interpolate(method='linear')\n",
    "    merged_df['fsc'] = merged_df['fsc'].interpolate(method='linear')\n",
    "    merged_df['gla_area_below_sl50'] = merged_df['gla_area_below_sl50'].interpolate(method='linear')\n",
    "    merged_df['gla_fsc'] = merged_df['gla_fsc'].interpolate(method='linear')\n",
    "\n",
    "    # 'gla_fsc_below_sl50' is 1 whenever 'gla_area_below_sl50' is 0\n",
    "    merged_df['gla_fsc_below_sl50'] = merged_df['gla_fsc_below_sl50'].mask(\n",
    "        merged_df['gla_area_below_sl50'] == 0, 1\n",
    "    )\n",
    "    merged_df['gla_fsc_below_sl50'] = merged_df['gla_fsc_below_sl50'].interpolate(method='linear')\n",
    "\n",
    "    # Identify rows where gla_fsc is NaN\n",
    "    na_mask = merged_df['gla_fsc'].isna()\n",
    "\n",
    "    # Set both columns to 0 where gla_fsc is NaN\n",
    "    merged_df.loc[na_mask, 'gla_area_below_sl50'] = 0\n",
    "    merged_df.loc[na_mask, 'gla_fsc_below_sl50'] = 0\n",
    "    merged_df.loc[na_mask, 'gla_fsc'] = 0\n",
    "\n",
    "    # for col in [\n",
    "    #     'SLA_East', 'SLA_North', 'SLA_South', 'SLA_West',\n",
    "    #     'fsc', 'gla_area_below_sl50', 'gla_fsc', 'gla_fsc_below_sl50'\n",
    "    # ]:\n",
    "    #     merged_df[col] = safe_interpolate(merged_df[col])\n",
    "\n",
    "    results.append(merged_df)\n",
    "\n",
    "# Concatenate all catchments into one full DataFrame\n",
    "df_full = pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# print(df_full)\n",
    "\n",
    "df = df_full.copy()\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "# drop the 'time_step' column\n",
    "df = df.drop(columns=['time_step'])\n",
    "\n",
    "# Plotting the data\n",
    "# ---------------------------------------------\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Loop through each unique catchment name\n",
    "for catchment_name in df['Name'].unique():\n",
    "    # Filter the DataFrame for the current catchment\n",
    "    catchment_data = df[df['Name'] == catchment_name]\n",
    "    # Plot the data for the current catchment\n",
    "    plt.plot(catchment_data['date'], catchment_data['SLA_East'], label=catchment_name)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Snowline Elevation by Aspect')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Snowline Elevation (m)')\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "# Add a legend\n",
    "# plt.legend()\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the maximum snowline elevation for each catchment\n",
    "for catchment_name in df['Name'].unique():\n",
    "    catchment_data = df[df['Name'] == catchment_name]\n",
    "    max_snowline = catchment_data['SLA_North'].max()\n",
    "    print(f\"Maximum snowline elevation for {catchment_name}: {max_snowline} m\")\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = '../data/fsc_sla_timeseries_gapfilled.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Data saved to {output_csv_path}\")\n",
    "# ---------------------------------------------\n",
    "# End of script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
