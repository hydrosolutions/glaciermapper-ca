{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=xrU9o8gWRrUPOlypK5_z8L-iaNC3XaSNDPPSVyuoiz8&tc=N6_tGgqBVO2hfzSw5iFqzE2y-KgtlHh0GklU6dTaPgs&cc=GXxe-6mCPDBxGcIkC3-5spYv6Yrmq3wG1FS_KlUikZU>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=xrU9o8gWRrUPOlypK5_z8L-iaNC3XaSNDPPSVyuoiz8&tc=N6_tGgqBVO2hfzSw5iFqzE2y-KgtlHh0GklU6dTaPgs&cc=GXxe-6mCPDBxGcIkC3-5spYv6Yrmq3wG1FS_KlUikZU</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate(auth_mode='notebook', force=True)\n",
    "ee.Initialize(project=\"ee-sahellakes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Snow Cover Mapping Notebook\n",
    "\n",
    "# This cell initializes Earth Engine and sets up paths and config\n",
    "\n",
    "import geemap\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Authenticate and initialize Earth Engine\n",
    "# try:\n",
    "#     ee.Initialize()\n",
    "# except Exception as e:\n",
    "#     ee.Authenticate(auth_mode='notebook')\n",
    "#     ee.Initialize()\n",
    "\n",
    "# Import your project-specific modules from `src`\n",
    "from src.modis_processing import load_modis, fill_modis_with_aqua, add_date_bands,create_decadal_composites, extract_year_ranges,process_interval\n",
    "from src.dem_processing import load_dem, classify_aspect, reproject_and_analyze_dem\n",
    "from src.snowline import get_snowline_elevation, calculate_glacier_metrics\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load AOI from RiverBasins_2023 asset\n",
    "# ---------------------------------------------\n",
    "RiverBasins_2023 = ee.FeatureCollection('users/hydrosolutions/RiverBasins_CA_Jan2023_simple1000')\n",
    "RiverBasins_2023 = RiverBasins_2023.map(lambda ft: ft.set('NAME', ee.String(ft.get('BASIN')).cat(ee.String('_')).cat(ee.String(ft.get('CODE')))))\n",
    "glims=ee.FeatureCollection(\"GLIMS/20230607\").filter(ee.Filter.eq('geog_area', \"Randolph Glacier Inventory; Umbrella RC for merging the RGI into GLIMS\"));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOI: {'type': 'Polygon', 'coordinates': [[[78.35558680529317, 42.841624935346175], [78.34886246924157, 42.82958091911706], [78.38582393379298, 42.79144665039145], [78.38308162476883, 42.74938845973651], [78.36565543091028, 42.739132472737566], [78.36738999652627, 42.73449054294478], [78.40500703170778, 42.72508188230116], [78.40886416962566, 42.710366813467374], [78.42423912934738, 42.69919676146307], [78.42649543944474, 42.69001100182323], [78.45619304986197, 42.68484290924801], [78.45496678270888, 42.676847760794644], [78.53783915075857, 42.69240559491001], [78.62981709232788, 42.703526603101125], [78.64310515220191, 42.687103715906815], [78.6819796237287, 42.678399517588296], [78.7454326670329, 42.686457096467194], [78.79148623308848, 42.69238772002842], [78.87993245601564, 42.69640981460532], [78.87640979374024, 42.7201055241284], [78.92885325597925, 42.72002523156944], [78.94693050268994, 42.695950536158875], [78.95391786273467, 42.686341204648464], [78.99936502354535, 42.67334734660159], [79.03066347869523, 42.64089846674175], [79.0711075189675, 42.62741861327105], [79.07782290127369, 42.60883306864663], [79.08775780204903, 42.602554652550644], [79.07267715575715, 42.56971341532836], [79.07765793141856, 42.54048846953656], [79.08181829111045, 42.49874232781894], [79.08033344185863, 42.47459183968445], [79.08617479594446, 42.47316943574702], [79.0735332477332, 42.44011851502033], [79.05639253823847, 42.43174434635261], [79.06032985769784, 42.40351380449559], [79.06856139436574, 42.40188179670944], [79.07569594051378, 42.38595828607868], [79.12267257264047, 42.376380172586394], [79.13281255382874, 42.37907343804989], [79.15610688381967, 42.38058951619181], [79.16689790732637, 42.402104747133734], [79.1945131859213, 42.407723159614015], [79.206258444223, 42.4201551735482], [79.2135802630614, 42.420395921242125], [79.21412877192094, 42.43114236165979], [79.20484930734499, 42.433773234218435], [79.1931798880193, 42.46630240333518], [79.2028695424863, 42.473526158461056], [79.19456664485186, 42.48956551950469], [79.19733131697326, 42.50640307576659], [79.19015656872904, 42.53672943991156], [79.18667402298966, 42.545139308155164], [79.14852645176077, 42.56475044772917], [79.16514554435177, 42.62892133373523], [79.14605164342068, 42.668411178264584], [79.14670717160524, 42.68724196223586], [79.1299587340527, 42.7040304577211], [79.07169167510851, 42.72811407748902], [79.04013903844228, 42.75160016692267], [79.00649065490731, 42.76986905024564], [79.00282974302958, 42.77739158477781], [78.95678952406757, 42.77964341109778], [78.92491142874681, 42.77884081059366], [78.86002704576477, 42.80069484250122], [78.84124530988991, 42.797827645958115], [78.80261169238995, 42.80837342188169], [78.78553331210365, 42.80085091905284], [78.77077372977874, 42.81561947173746], [78.73609527840314, 42.82039963124027], [78.70474776784435, 42.82308843535467], [78.69476389550526, 42.81404536000539], [78.6622079990936, 42.816823442757], [78.64570037621027, 42.83622052219088], [78.63716567401748, 42.84414430725323], [78.60569779345342, 42.85401233224817], [78.58876214211143, 42.84734598253643], [78.56925354334342, 42.86338981333428], [78.55973784152995, 42.86508875489021], [78.54083565197142, 42.869382795964476], [78.52520652374065, 42.886889224362605], [78.5020147650544, 42.895129703308676], [78.49183015548478, 42.90048506275966], [78.47381990297184, 42.8947907860482], [78.4642907447993, 42.89648526883846], [78.4582263588598, 42.888895850228664], [78.42987096228846, 42.87955851976537], [78.38745600483058, 42.88217600113424], [78.37108664832029, 42.8871300543074], [78.35587660984324, 42.859595123345045], [78.35331713354026, 42.85080621561754], [78.35558680529317, 42.841624935346175]]]}\n",
      "intervals: [{'type': 'Date', 'value': 1746057600000}, {'type': 'Date', 'value': 1746950400000}]\n",
      "sample_img: {'type': 'Image', 'bands': [{'id': 'value', 'data_type': {'type': 'PixelType', 'precision': 'double', 'min': 0, 'max': 255}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}], 'properties': {'system:time_start': 1752192000000, 'Year-Month-Day': '2025-07-11', 'system:index': '0'}}\n",
      "Snowline elevation by aspect: {'type': 'Feature', 'geometry': None, 'properties': {'East': 4127.176967399908, 'North': 4127.176967399908, 'South': 4127.176967399908, 'West': 4127.176967399908, 'mixed': 4127.176967399908}}\n",
      "Fractional Snow Cover 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# filter by code 16230\n",
    "aoi = RiverBasins_2023.filter(ee.Filter.eq('CODE', '15002')).geometry()\n",
    "print('AOI:', aoi.getInfo())\n",
    "\n",
    "start_year = 2025\n",
    "end_year = 2025\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load and process MODIS\n",
    "# ---------------------------------------------\n",
    "intervals = ee.List(extract_year_ranges(ee.List.sequence(start_year, end_year), 10)\n",
    "    .iterate(lambda list, previous: ee.List(previous).cat(ee.List(list)), ee.List([])))\n",
    "\n",
    "# print('intervals:',intervals)\n",
    "print('intervals:', ee.List(intervals.get(12)).getInfo())\n",
    "\n",
    "# Load all MODIS data for the entire time span\n",
    "start_date = ee.Date.fromYMD(start_year, 1, 1)\n",
    "end_date = ee.Date.fromYMD(end_year, 12, 31)\n",
    "terra_coll = load_modis(aoi)\n",
    "\n",
    "# Create filled MODIS snow cover fraction collection\n",
    "mscf = terra_coll.map(lambda img: fill_modis_with_aqua(img))\n",
    "# print('mscf:', mscf.first().getInfo())\n",
    "\n",
    "# time_intervals_all should be an ee.List of [start, end] ee.Date pairs\n",
    "modis_ic = ee.ImageCollection(intervals.slice(19,20).map(lambda list:process_interval(mscf, list)))\n",
    "# print('modis_ic:', modis_ic.filterBounds(aoi).getInfo())\n",
    "\n",
    "# modis_ic = create_decadal_composites(aoi, start_year, end_year, agg_interval=10)\n",
    "# print('modis_ic:', modis_ic.filterBounds(aoi).first().getInfo())\n",
    "# terra, aqua = load_modis(aoi, start_date, end_date)\n",
    "# filled_modis = terra.map(lambda img: fill_modis_with_aqua(img, aqua))\n",
    "# filled_modis = filled_modis.map(add_date_bands)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load DEM and classify terrain aspect\n",
    "# ---------------------------------------------\n",
    "dem = load_dem()\n",
    "modisProjection = modis_ic.filterBounds(aoi).first().projection()\n",
    "scale = 500\n",
    "tile_scale = 2\n",
    "sc_th = 50\n",
    "aspect_keys = ['East', 'North', 'South', 'West', 'mixed']\n",
    "# Reproject and analyze DEM\n",
    "reprojected_dem, min_dem_dict, max_dem_dict, n_grid = reproject_and_analyze_dem(dem, modisProjection, aoi, scale, tile_scale, aspect_keys)\n",
    "# print('reprojected_dem:', reprojected_dem.getInfo())\n",
    "\n",
    "\n",
    "aspects, aspect_coded = classify_aspect(dem, modisProjection, scale)\n",
    "# print('aspects:', aspects.bandNames().getInfo())\n",
    "# print('n_grid:', n_grid.getInfo())\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Snowline elevation estimation from first MODIS image\n",
    "# ---------------------------------------------\n",
    "sample_img = ee.Image(modis_ic.filterDate(start_date, end_date).first())\n",
    "\n",
    "\n",
    "# Test on a problematic summer image\n",
    "sample_img = modis_ic.filterDate('2025-07-01', '2025-08-01').first()\n",
    "\n",
    "print('sample_img:', sample_img.getInfo())\n",
    "# Get snowline elevation and statistics\n",
    "snowline_stats,fsc = get_snowline_elevation(sample_img, reprojected_dem, aspect_coded, aoi,min_dem_dict, max_dem_dict,n_grid, \n",
    "                                                      scale=500, scale_dem=500, sc_th=50, canny_threshold=0.7, \n",
    "                           canny_sigma=0.7, ppha=10, tile_scale=1, point2sample=1000, aspectKeys=['East', 'North', 'South', 'West', 'mixed'])\n",
    "  \n",
    "print('Snowline elevation by aspect:', snowline_stats.getInfo())\n",
    "print('Fractional Snow Cover', fsc.getInfo())\n",
    "\n",
    "glacier_metrics=calculate_glacier_metrics(glims, aoi, sample_img,sc_th, snowline_stats , dem, aspect_keys, tile_scale, aspects)\n",
    "# Access individual EE values from the dictionary and get their info\n",
    "# print('Glacier SCF:', glacier_metrics['glims_fsc'].getInfo())\n",
    "# print('Glacier SCF below snowline:', glacier_metrics['glims_fsc_below_sl'].getInfo())\n",
    "# print('Glacier area below snowline:', glacier_metrics['glims_area_below_sl'].getInfo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing catchment ISSYKUL_15054...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15002...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15013...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15016...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15020...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15022...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15025...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15030...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15034...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15039...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15040...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15044...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15045...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15049...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15051...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15057...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15064...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15069...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15070...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15081...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15083...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ISSYKUL_15090...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17461...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17043...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17045...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17047...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17050...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17058...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17059...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17062...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17073...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17078...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17082...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17100...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17101...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17110...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17137...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17150...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17185...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17202...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17288...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17325...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17329...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17338...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17344...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17391...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17453...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17459...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17462...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_16223...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SURKHANDARYA_17194...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SURKHANDARYA_17211...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SURKHANDARYA_17223...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17231...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17236...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17257...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17260...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17275...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17279...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment QASHQADARYA_17464...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-0.000-4M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-0.000-6M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-1.1L0-7A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-0.000-3M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_10-1.L00-1T...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_11-0.000-4M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SHIRINTAGAB_11-1.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment BALKH_12-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment BALKH_12-0.000-9M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment BALKH_12-0.000-10M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment BALKH_12-1.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KHULM_13-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KHULM_13-0.000-2M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-1.1L0-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-0.000-3M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-0.000-6M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-1.L00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-10.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOKCHA_15-10.R00-2A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-2M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-3M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-4M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-5S...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-6M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-0.000-8M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-1.R00-2A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-1.R00-5A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-5.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-9.5R0-1T...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-9.R00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-9.R00-6T...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KUNDUZ_14-9.R00-8A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17089...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17102...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17106...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17107...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17109...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17111...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17113...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17114...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17115...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17116...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17119...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17121...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment VAKSH_17122...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17125...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17126...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17141...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17147...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17157...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17160...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17165...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17169...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17170...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17172...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17176...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17177...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17178...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment KOFARNIKHAN_17179...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17324...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17333...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17335...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17341...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17342...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_17343...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ZERAFSHAN_70003...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17052...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17053...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17057...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17061...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17063...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17064...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17065...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17067...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17068...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17072...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17074...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17077...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_17081...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_70001...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment PYANDZH_70002...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15102...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15149...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15171...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15189...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15194...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15212...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15214...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15215...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHU_15216...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15256...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15259...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15261...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15278...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15283...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15285...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15287...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15290...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15292...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment TALAS_15312...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16133...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16936...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16326...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16340...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16363...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16374...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16375...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment ARYS_16390...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16055...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16059...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16068...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16070...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16076...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16101...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16121...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16127...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16134...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16135...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16136...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16139...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16143...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16146...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16151...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16153...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16158...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16159...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16169...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16176...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16487...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16510...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16107...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16262...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16279...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16290...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16298...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16300...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_16924...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16938...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60048...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60032...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60033...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60034...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60035...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60036...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60037...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60038...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60039...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60040...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment CHIRCHIK_60041...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60042...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60043...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60044...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60045...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_60046...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment AKHANGARAN_16230...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16057...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16072...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16074...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16075...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16080...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16081...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16082...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16085...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16087...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16088...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16093...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16096...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16100...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16103...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_16105...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16124...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16137...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16154...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16155...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16160...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16161...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16162...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16163...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16164...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16175...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16187...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16192...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16193...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16198...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16202...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16205...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16210...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_16211...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60002...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60003...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60005...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60006...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60007...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60008...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60009...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60010...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60011...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60012...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60013...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60014...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60015...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60016...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60017...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60018...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60019...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60020...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment NARYN_60021...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60022...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60023...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60024...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60025...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60026...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60029...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment SYR_DARYA_60031...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-0.000-5M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-1.000-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-2.000-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-3.000-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-4.R00-8A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment MURGHAB_9-5.L00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-1M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-2M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-3S...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-4M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-5M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-7M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-0.000-9M...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-1.R00-9T...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-3.L00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-3.L00-6A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-11.L00-1A...\n",
      "Export started for year 2025 and month 8.\n",
      "Processing catchment HARIRUD_8-2.R00-3A...\n",
      "Export started for year 2025 and month 8.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Loop over each basin and export results for a given month (or several months)\n",
    "year=2025\n",
    "catchment_names=RiverBasins_2023.aggregate_array('NAME').getInfo()\n",
    "#get the current month\n",
    "current_month = datetime.datetime.now().month  # returns 1-12\n",
    "\n",
    "for catchment_name in catchment_names:\n",
    "\n",
    "    print(f\"Processing catchment {catchment_name}...\")\n",
    "    \n",
    "    # filter by code 16230\n",
    "    aoi = RiverBasins_2023.filter(ee.Filter.eq('NAME', catchment_name)).geometry()\n",
    "\n",
    "    # Load MODIS data for the year\n",
    "    modis_ic = create_decadal_composites(aoi, year, year, agg_interval=10)\n",
    "\n",
    "    # Define start and end dates for the year: run from the beginning to the previous month from now\n",
    "    start_date = ee.Date.fromYMD(year, 1, 1)\n",
    "    end_date = ee.Date.fromYMD(year, current_month - 1, 31)\n",
    "    \n",
    "    # Filter by date\n",
    "    modis_ic = ee.ImageCollection(modis_ic).filter(ee.Filter.And(ee.Filter.calendarRange(year, year, 'year'), ee.Filter.calendarRange(1, current_month - 1, 'month')))\n",
    "    modisProjection = modis_ic.filterBounds(aoi).first().projection()\n",
    "    reprojected_dem, min_dem_dict, max_dem_dict, n_grid = reproject_and_analyze_dem(dem, modisProjection, aoi, scale, tile_scale, aspect_keys)\n",
    "    aspects, aspect_coded = classify_aspect(dem, modisProjection, scale)    \n",
    "    \n",
    "    # Function to process each image and create a feature with properties\n",
    "    def create_feature_with_properties(img):\n",
    "        # Get snowline elevation for this image\n",
    "        current_snowline_stats, current_fsc = get_snowline_elevation(\n",
    "            img, reprojected_dem, aspect_coded, aoi, min_dem_dict, max_dem_dict, n_grid,\n",
    "            scale=500, scale_dem=500, sc_th=50, canny_threshold=0.7,\n",
    "            canny_sigma=0.7, ppha=10, tile_scale=1, point2sample=1000, \n",
    "            aspectKeys=aspect_keys\n",
    "        )\n",
    "        \n",
    "        # Calculate glacier metrics\n",
    "        current_glacier_metrics = calculate_glacier_metrics(\n",
    "            glims, aoi, img, sc_th, current_snowline_stats, dem, aspect_keys, tile_scale, aspects\n",
    "        )\n",
    "        \n",
    "        # Create a dictionary with aspect-specific values\n",
    "        aspect_dict = {}\n",
    "        base = ee.String('SLA_')\n",
    "        \n",
    "        # Add properties for each aspect (excluding 'mixed')\n",
    "        for i, aspect in enumerate(aspect_keys[:-1]):\n",
    "            aspect_dict[base.cat(aspect)] = current_snowline_stats.get(aspect)\n",
    "        \n",
    "        # Get date info\n",
    "        img_date = ee.Date(img.get('system:time_start'))\n",
    "        img_year = img_date.get('year')\n",
    "        img_decade = ee.Number(img_date.get('day')).add(2).divide(10).ceil()\n",
    "        \n",
    "        # Create feature with all properties\n",
    "        feature = ee.Feature(None).set(\n",
    "            'Year-Month-Day', img_date.format('YYYY-MM-dd'),\n",
    "            'year', img_year,\n",
    "            'decade', img_decade,\n",
    "            'gla_fsc', current_glacier_metrics['glims_fsc'],\n",
    "            'gla_fsc_below_sl50', current_glacier_metrics['glims_fsc_below_sl'],\n",
    "            'gla_area_below_sl50', current_glacier_metrics['glims_area_below_sl'],\n",
    "            'fsc', current_fsc\n",
    "        )\n",
    "        \n",
    "        # Add aspect-specific properties\n",
    "        for key, value in aspect_dict.items():\n",
    "            feature = feature.set(key, value)\n",
    "            \n",
    "        return feature\n",
    "    \n",
    "    # Apply the function to each image in the collection\n",
    "    aoi_mean_tmp = modis_ic.map(create_feature_with_properties)\n",
    "    \n",
    "    # Add geometry to features (because null geometry can't be exported)\n",
    "    joined = aoi_mean_tmp.map(lambda ft: ee.Feature(aoi.centroid(1000)).copyProperties(ft))\n",
    "    \n",
    "    # Sort by glacier snow cover below snowline\n",
    "    table_to_export = joined#.sort('gla_fsc_below_sl50', False)\n",
    "    \n",
    "    export_layer_name = 'decadal_SLA'  # Modify as needed\n",
    "    \n",
    "    # Create year_month string for asset naming\n",
    "    year_month = f\"{year}_{current_month-1:02d}\"\n",
    "    \n",
    "    # Export to asset\n",
    "    task = ee.batch.Export.table.toAsset(\n",
    "        collection=ee.FeatureCollection(table_to_export).set('NAME', catchment_name),\n",
    "        description=f\"{export_layer_name}_{catchment_name.replace('.', '')}_{year_month}\",\n",
    "        assetId=f\"projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/{export_layer_name}_{catchment_name.replace('.', '')}_{year_month}\"\n",
    "    )\n",
    "    \n",
    "    # # Start the export task\n",
    "    task.start()\n",
    "    print(f\"Export started for year {year} and month {current_month-1}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through years 2001 to 2024 (annual assets)\n",
    "for year in range(start_year, end_year + 1):\n",
    "\n",
    "    print(f\"Processing year {year}...\")\n",
    "    \n",
    "    # Define start and end dates for the year\n",
    "    start_date = ee.Date.fromYMD(year, 1, 1)\n",
    "    end_date = ee.Date.fromYMD(year, 12, 31)\n",
    "    \n",
    "    # Load MODIS data for the year\n",
    "    modis_ic = create_decadal_composites(aoi, year, year, agg_interval=10)\n",
    "    \n",
    "    # Load and process DEM if needed (or reuse from outside the loop if not changing)\n",
    "    dem = load_dem()\n",
    "    modisProjection = modis_ic.filterBounds(aoi).first().projection()\n",
    "    \n",
    "    # Function to process each image and create a feature with properties\n",
    "    def create_feature_with_properties(img):\n",
    "        # Get snowline elevation for this image\n",
    "        current_snowline_stats, current_fsc = get_snowline_elevation(\n",
    "            img, reprojected_dem, aspect_coded, aoi, min_dem_dict, max_dem_dict, n_grid,\n",
    "            scale=500, scale_dem=500, sc_th=50, canny_threshold=0.7,\n",
    "            canny_sigma=0.7, ppha=10, tile_scale=1, point2sample=1000, \n",
    "            aspectKeys=aspect_keys\n",
    "        )\n",
    "        \n",
    "        # Calculate glacier metrics\n",
    "        current_glacier_metrics = calculate_glacier_metrics(\n",
    "            glims, aoi, img, sc_th, current_snowline_stats, dem, aspect_keys, tile_scale, aspects\n",
    "        )\n",
    "        \n",
    "        # Create a dictionary with aspect-specific values\n",
    "        aspect_dict = {}\n",
    "        base = ee.String('SLA_')\n",
    "        \n",
    "        # Add properties for each aspect (excluding 'mixed')\n",
    "        for i, aspect in enumerate(aspect_keys[:-1]):\n",
    "            aspect_dict[base.cat(aspect)] = current_snowline_stats.get(aspect)\n",
    "        \n",
    "        # Get date info\n",
    "        img_date = ee.Date(img.get('system:time_start'))\n",
    "        img_year = img_date.get('year')\n",
    "        img_decade = ee.Number(img_date.get('day')).add(2).divide(10).ceil()\n",
    "        \n",
    "        # Create feature with all properties\n",
    "        feature = ee.Feature(None).set(\n",
    "            'Year-Month-Day', img_date.format('YYYY-MM-dd'),\n",
    "            'year', img_year,\n",
    "            'decade', img_decade,\n",
    "            'gla_fsc', current_glacier_metrics['glims_fsc'],\n",
    "            'gla_fsc_below_sl50', current_glacier_metrics['glims_fsc_below_sl'],\n",
    "            'gla_area_below_sl50', current_glacier_metrics['glims_area_below_sl'],\n",
    "            'fsc', current_fsc\n",
    "        )\n",
    "        \n",
    "        # Add aspect-specific properties\n",
    "        for key, value in aspect_dict.items():\n",
    "            feature = feature.set(key, value)\n",
    "            \n",
    "        return feature\n",
    "    \n",
    "    # Apply the function to each image in the collection\n",
    "    aoi_mean_tmp = modis_ic.map(create_feature_with_properties)\n",
    "    \n",
    "    # Add geometry to features (because null geometry can't be exported)\n",
    "    joined = aoi_mean_tmp.map(lambda ft: ee.Feature(aoi.centroid(1000)).copyProperties(ft))\n",
    "    \n",
    "    # Sort by glacier snow cover below snowline\n",
    "    table_to_export = joined#.sort('gla_fsc_below_sl50', False)\n",
    "    \n",
    "    # Set catchment name (assuming it's defined elsewhere)\n",
    "    catchment_name = 'AKHANGARAN_16230'  # Modify as needed\n",
    "    export_layer_name = 'decadal_SLA'  # Modify as needed\n",
    "    \n",
    "    # Export to asset\n",
    "    task = ee.batch.Export.table.toAsset(\n",
    "        collection=ee.FeatureCollection(table_to_export).set('NAME', catchment_name),\n",
    "        description=f\"{export_layer_name}_{catchment_name.replace('.', '')}_Year{year}\",\n",
    "        assetId=f\"projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/{export_layer_name}_{catchment_name.replace('.', '')}_Year{year}\"\n",
    "    )\n",
    "    \n",
    "    # Start the export task\n",
    "    task.start()\n",
    "    print(f\"Export started for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assets updated in the last 10 days: 13\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_AKHANGARAN_16230_Year2025\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15002_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15013_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15016_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15020_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15022_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15025_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15030_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15034_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15040_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15044_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15045_2025_08\n",
      "ID: projects/ee-hydro4u/assets/snow_CentralAsia/Folder4SLA_v4/decadal_SLA_ISSYKUL_15054_2025_08\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Once exports are completed, share all assets publically\n",
    "assetdir = f\"projects/ee-hydro4u/assets/{'snow_CentralAsia/Folder4SLA_v4'}\"\n",
    "assets = ee.data.listAssets({\"parent\": assetdir})[\"assets\"]\n",
    "\n",
    "# Get the current UTC time and subtract 10 days\n",
    "cutoff_time = datetime.now(timezone.utc) - timedelta(days=10)\n",
    "\n",
    "# Filter assets updated in the last 10 days\n",
    "recent_assets = [\n",
    "    asset for asset in assets\n",
    "    if 'updateTime' in asset and datetime.fromisoformat(asset['updateTime'].replace('Z', '+00:00')) > cutoff_time\n",
    "]\n",
    "\n",
    "print(f\"Assets updated in the last 10 days: {len(recent_assets)}\")\n",
    "for table in recent_assets:\n",
    "    id = table['id']\n",
    "    asset = ee.data.getAsset(id)\n",
    "    ee.data.setIamPolicy(id, {\n",
    "        'bindings': [{'role': 'roles/owner', 'members': ['user:workshop.gee@gmail.com']},\n",
    "                        {'role': 'roles/viewer', 'members': ['allUsers']}]})\n",
    "    print('ID: ' + id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered on the AOI\n",
    "Map = geemap.Map(center=[aoi.centroid().coordinates().get(1).getInfo(), \n",
    "                         aoi.centroid().coordinates().get(0).getInfo()], \n",
    "                 zoom=8)\n",
    "\n",
    "# provide a map with the sample_img\n",
    "# Visualization parameters\n",
    "vis_params = {\n",
    "    'min': 0,\n",
    "    'max': 100,\n",
    "    'palette': ['blue', 'white']\n",
    "}\n",
    "\n",
    "# Add the sample image to the map\n",
    "Map.addLayer(sample_img, vis_params, 'Sample Image')\n",
    "\n",
    "# Add the AOI boundary to the map\n",
    "Map.addLayer(aoi, {'color': 'red'}, 'AOI')\n",
    "# Display the map in the notebook\n",
    "Map.addLayerControl()  # This adds layer control to the map\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the exported data: data/fsc_sla_timeseries.csv\n",
    "# ---------------------------------------------\n",
    "# Load the exported data from Google Earth Engine\n",
    "# ---------------------------------------------\n",
    "# Define the path to the exported CSV file\n",
    "exported_csv_path = '../data/fsc_sla_timeseries.csv'\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(exported_csv_path)\n",
    "\n",
    "# Convert the 'Year-Month-Day' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['Year-Month-Day'])\n",
    "# Sort the DataFrame by 'Name' and then by 'date'\n",
    "df = df.sort_values(by=['Name', 'date'])\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['Name', 'date'], keep='last')\n",
    "# Calculate the time-steps between data points of the same catchment\n",
    "df['time_step'] = df.groupby('Name')['date'].diff().dt.days\n",
    "# Fill NaN values in the 'time_step' column with 0\n",
    "df['time_step'] = df['time_step'].fillna(0)\n",
    "\n",
    "# drop the 'system:index' column\n",
    "df = df.drop(columns=['system:index'])\n",
    "# drop the 'Year-Month-Day' column\n",
    "df = df.drop(columns=['Year-Month-Day'])\n",
    "\n",
    "# Define the decades in a month (1st, 2nd, and 3rd decade)\n",
    "decades = [1, 2, 3]\n",
    "results = []\n",
    "\n",
    "def safe_interpolate(series):\n",
    "    \"\"\"\n",
    "    Interpolates a series only if both previous and next values are available.\n",
    "    \"\"\"\n",
    "    s = series.copy()\n",
    "    for i in range(1, len(s) - 1):\n",
    "        if pd.isna(s.iloc[i]) and not pd.isna(s.iloc[i - 1]) and not pd.isna(s.iloc[i + 1]):\n",
    "            s.iloc[i] = (s.iloc[i - 1] + s.iloc[i + 1]) / 2\n",
    "    return s\n",
    "\n",
    "\n",
    "# Loop through each unique catchment name\n",
    "for catchment_name in df['Name'].unique():\n",
    "    catchment_data = df[df['Name'] == catchment_name].copy()  # ← copy is essential\n",
    "\n",
    "    # Extract date parts safely\n",
    "    catchment_data['year'] = catchment_data['date'].dt.year\n",
    "    catchment_data['month'] = catchment_data['date'].dt.month\n",
    "\n",
    "    # Define decades\n",
    "    decades = [1, 2, 3]\n",
    "\n",
    "    # Get year range\n",
    "    years = range(catchment_data['year'].min(), 2024 + 1)\n",
    "    months = range(1, 13)\n",
    "\n",
    "    # Build full time grid\n",
    "    combinations = pd.DataFrame([\n",
    "        (year, month, decade) for year in years for month in months for decade in decades\n",
    "    ], columns=['year', 'month', 'decade'])\n",
    "\n",
    "    # Merge with actual data to find missing\n",
    "    merged_df = pd.merge(combinations, catchment_data, on=['year', 'month', 'decade'], how='left')\n",
    "\n",
    "    # Preserve catchment name\n",
    "    merged_df['Name'] = catchment_name\n",
    "    # Define a mapping for which day each decade starts on\n",
    "    decade_start_days = {1: 1, 2: 11, 3: 21}\n",
    "\n",
    "    # Apply to your merged_df to create a valid date\n",
    "    merged_df['date'] = pd.to_datetime({\n",
    "        'year': merged_df['year'],\n",
    "        'month': merged_df['month'],\n",
    "        'day': merged_df['decade'].map(decade_start_days)\n",
    "    })\n",
    "\n",
    "    # Fill NaN values in 'Basin' and 'Code' and '.geo' columns\n",
    "    merged_df['Basin'] = merged_df['Basin'].fillna(method='ffill')\n",
    "    merged_df['Code'] = merged_df['Code'].fillna(method='ffill')\n",
    "    merged_df['.geo'] = merged_df['.geo'].fillna(method='ffill')\n",
    "    # Fill NaN values in 'SLA_East', 'SLA_North', 'SLA_South', 'SLA_West', 'fsc', 'gla_area_below_sl50', and 'gla_fsc' columns\n",
    "    # interpolate between previous and next values\n",
    "    merged_df['SLA_East'] = merged_df['SLA_East'].interpolate(method='linear')\n",
    "    merged_df['SLA_North'] = merged_df['SLA_North'].interpolate(method='linear')\n",
    "    merged_df['SLA_South'] = merged_df['SLA_South'].interpolate(method='linear')\n",
    "    merged_df['SLA_West'] = merged_df['SLA_West'].interpolate(method='linear')\n",
    "    merged_df['fsc'] = merged_df['fsc'].interpolate(method='linear')\n",
    "    merged_df['gla_area_below_sl50'] = merged_df['gla_area_below_sl50'].interpolate(method='linear')\n",
    "    merged_df['gla_fsc'] = merged_df['gla_fsc'].interpolate(method='linear')\n",
    "\n",
    "    # 'gla_fsc_below_sl50' is 1 whenever 'gla_area_below_sl50' is 0\n",
    "    merged_df['gla_fsc_below_sl50'] = merged_df['gla_fsc_below_sl50'].mask(\n",
    "        merged_df['gla_area_below_sl50'] == 0, 1\n",
    "    )\n",
    "    merged_df['gla_fsc_below_sl50'] = merged_df['gla_fsc_below_sl50'].interpolate(method='linear')\n",
    "\n",
    "    # Identify rows where gla_fsc is NaN\n",
    "    na_mask = merged_df['gla_fsc'].isna()\n",
    "\n",
    "    # Set both columns to 0 where gla_fsc is NaN\n",
    "    merged_df.loc[na_mask, 'gla_area_below_sl50'] = 0\n",
    "    merged_df.loc[na_mask, 'gla_fsc_below_sl50'] = 0\n",
    "    merged_df.loc[na_mask, 'gla_fsc'] = 0\n",
    "\n",
    "    # for col in [\n",
    "    #     'SLA_East', 'SLA_North', 'SLA_South', 'SLA_West',\n",
    "    #     'fsc', 'gla_area_below_sl50', 'gla_fsc', 'gla_fsc_below_sl50'\n",
    "    # ]:\n",
    "    #     merged_df[col] = safe_interpolate(merged_df[col])\n",
    "\n",
    "    results.append(merged_df)\n",
    "\n",
    "# Concatenate all catchments into one full DataFrame\n",
    "df_full = pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# print(df_full)\n",
    "\n",
    "df = df_full.copy()\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "# drop the 'time_step' column\n",
    "df = df.drop(columns=['time_step'])\n",
    "\n",
    "# Plotting the data\n",
    "# ---------------------------------------------\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Loop through each unique catchment name\n",
    "for catchment_name in df['Name'].unique():\n",
    "    # Filter the DataFrame for the current catchment\n",
    "    catchment_data = df[df['Name'] == catchment_name]\n",
    "    # Plot the data for the current catchment\n",
    "    plt.plot(catchment_data['date'], catchment_data['SLA_East'], label=catchment_name)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Snowline Elevation by Aspect')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Snowline Elevation (m)')\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "# Add a legend\n",
    "# plt.legend()\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the maximum snowline elevation for each catchment\n",
    "for catchment_name in df['Name'].unique():\n",
    "    catchment_data = df[df['Name'] == catchment_name]\n",
    "    max_snowline = catchment_data['SLA_North'].max()\n",
    "    print(f\"Maximum snowline elevation for {catchment_name}: {max_snowline} m\")\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = '../data/fsc_sla_timeseries_gapfilled.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Data saved to {output_csv_path}\")\n",
    "# ---------------------------------------------\n",
    "# End of script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
